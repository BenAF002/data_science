{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfdd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d4e5f",
   "metadata": {},
   "source": [
    "**Definition 1** (Associative Memory)\\\n",
    "Given a set of keys $\\mathcal K\\in \\reals^{d_k}$ and values $\\mathcal V \\in \\reals^{d_v}$, associative memory is an *operator* $\\mathcal M : \\mathcal K \\mapsto \\mathcal V$\\\n",
    "Objective $\\tilde{\\mathcal{L}}(\\cdot, \\cdot)$ measures the quality of the mapping. The optimal mapping given an objective is defined as:\n",
    "$$\\mathcal M^* = \\argmin_\\mathcal{M} \\tilde{\\mathcal{L}} (\\mathcal M(\\mathcal K), \\mathcal V)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1dac0",
   "metadata": {},
   "source": [
    "This definition seems to simply conceptualize model parameters (like weights) as defining a mapping between any arbitrary input keys and values. So far, this is just explicating what we already know about parameters (especially w.r.t. matrices of weights which literally are representations of linear maps). I suppose it is just a bit more general because we are stating that the objective is to pick the optimal mapping that minimizes the objective, which could include architectural considerations like rgularization and non-linearities, rather than just the learnable parameters.\n",
    "\n",
    "For comprehension: \"Memory\" $\\approxeq$ \"Map\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9151a0",
   "metadata": {},
   "source": [
    "### Simple MLP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bea3e",
   "metadata": {},
   "source": [
    "Consider a **1-Layer** MLP with parameters $W$. The optimization problem is: $$W^* = \\argmin_W \\mathcal L (W, \\mathcal D_{tr})$$\n",
    "Where $\\mathcal D_{tr}$ is the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f884f4d",
   "metadata": {},
   "source": [
    "Then the gradient descent update rule is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "& W_{t+1} = W_t - \\eta \\nabla_{W_t} \\mathcal L (W_t, x_{t+1}) \\\\\n",
    "& \\ \\ \\ \\  \\ \\  \\ \\ \\ = W_t - \\eta \\nabla_{y_{t+1}} \\mathcal L (W_t, x_{t+1}) \\otimes x_{t+1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb806e",
   "metadata": {},
   "source": [
    "The second expression comes from noting that $\\frac{\\partial \\mathcal L}{\\partial w_i} = \\frac{\\partial \\mathcal L}{\\partial y_{t+1}} \\frac{\\partial y_{t+1}}{\\partial w_i}$ and $\\frac{\\partial y_{t+1}}{\\partial w_i} = x_{t+1}$ **and that** the paper denotes $y_t \\coloneqq \\hat y$\\\n",
    "**NOTE:** This is seemingly in the context of *online* or ***Streaming*** learning, wherein each input has a time-step $t$. The model state at time $t+1$ begins with weights from time $t$, i.e. $W_t$. Then, it predicts $y_{t+1}$ using the weights $W_t$, **and then** updates them to get $W_{t+1}$. Hence why $y_{t+1}$ is used in the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59901469",
   "metadata": {},
   "source": [
    "**Reformulation**\\\n",
    "Now, rewrite the optimization problem as the *regularized* minimization of the inner-product between the predicted output and the gradient:\n",
    "$$\n",
    "\\begin{align*}\n",
    "& W_{t+1} = \\argmin_W \\langle Wx_{1+1}, \\nabla_{y_{t+1}} \\mathcal L (W_t, x_{t+1}) \\rangle + \\frac{1}{2\\eta} \\|W - W_t \\|_2^2 \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\argmin_W \\langle y_{t+1}, u_{t+1} \\rangle + \\frac{1}{2\\eta} \\|W - W_t \\|_2^2 \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d05f0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
