{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of Norm:**\n",
    "- A norm on a vector space $V$ is a function $\\|\\cdot \\| : V \\rightarrow \\reals$ which assigns each vector $\\mathbb{x}$ its length $\\|\\mathbb{x}\\| \\in \\reals$, such that the following conditions hold for all $\\lambda \\in \\reals, \\ \\mathbb{x}, \\mathbb{y} \\in V$:\n",
    "    - ***Absolute Homogeneity***: $\\|\\lambda\\mathbb{x}\\| = |\\lambda|\\|\\mathbb{x}\\|$\n",
    "        - i.e. the norm of its scaled version is equal to the absolute value of its scaled norm\n",
    "    - ***Triangle Inequality***: $\\|\\mathbb{x} + \\mathbb{y}\\| \\le \\|\\mathbb{x}\\| + \\| \\mathbb{y}\\|$\n",
    "        - i.e. the norm of a sum of vectors is equal to the sum of their norms\n",
    "        - named as such bc in geometry, for any triangle, the sum of any two sides must be greater than or equal to the length of the remaining side\n",
    "    - ***Positive Definite***: $\\|\\mathbb{x}\\| \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$\\mathcal{l}_1$ - \"Manhattan\" Norm***\n",
    "$$\\|\\mathbb{x}\\|_1 \\coloneqq \\sum_{i=1}^n|x_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$\\mathcal{l}_2$ - \"Euclidean\" Norm***\n",
    "$$\\|\\mathbb{x}\\|_2 \\coloneqq \\sqrt{\\sum_{i=1}^n x_i^2} = \\mathbb{x}^\\intercal \\mathbb{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Inner Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot Product:**\n",
    "- For vectors $x$ and $y$:\n",
    "$$\\mathbb{x}^\\intercal \\mathbb{y} = \\sum_{i=1}^n x_i y_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Inner Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***Bilinear Mapping*** $\\Omega$ is a mapping with two arguments that is linear in each argument. For a vector space $V$, the following conditions hold for such a mapping:\n",
    "$$\\Omega(\\lambda x + \\psi y, \\ z) = \\lambda \\Omega(x, z) + \\psi\\Omega(y, z) \\\\ \\Omega(x, \\ \\lambda y + \\psi z) = \\lambda \\Omega(x, y) + \\psi\\Omega(x, z)$$\n",
    "Look at this closesly, $\\Omega$ has two arguments. The first condition above states that $\\Omega$ is linear in its first argument while the second condition states that $\\Omega$ is linear in its second argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Definitions\n",
    "For a *bilinear mapping* $\\Omega:V \\times V \\rightarrow \\reals$ that takes two vectors and maps them onto a real number:\n",
    "- ***Symmetric***\n",
    "    - $\\Omega$ is symmetric if $\\Omega(x,y) = \\Omega(y,x) \\ \\ \\forall x,y \\in V$\n",
    "    - i.e. $\\Omega$ is symmetric if the order of its arguments does not matter\n",
    "- ***Positive Definite***\n",
    "    - $\\Omega$ is positive definite if:\n",
    "    $$ \\forall x\\in V \\setminus \\{\\mathbb{0}\\} \\ : \\ \\Omega(x, x) > 0, \\ \\Omega(\\mathbb{0}, \\mathbb{0}) = 0$$\n",
    "    - i.e. $\\Omega$ is positive definite if it maps to only positive numbers in $\\reals$ for all vectors in $V$ other than the zero vector $\\mathbb{0}$\n",
    "    - Note that this is only for $\\Omega(\\bf x, x)$, *not* for $\\Omega(\\bf x, y), \\ x \\ne y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of Inner Product:**\n",
    "- A bilinear mapping $\\Omega \\ : \\ V \\times V \\rightarrow \\reals$ that takes two vectors and maps them onto a real number is called an *inner product* if it is both **symmetric** and **positive definite**\n",
    "    - Inner products on vectors $x$ and $y$ are typically denoted $\\langle x, y \\rangle$\n",
    "    - The *Inner Product Space* of a vector space $V$ is denoted $\\big(V, \\langle\\cdot, \\cdot\\rangle\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric, Positive Definite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vector space $V$ and inner product $\\langle \\cdot, \\cdot \\rangle : \\ V \\times V \\rightarrow \\reals$, ordered basis $B=(\\mathbf{b}_1, ..., \\mathbf{b}_n)$ of $V$, and vectors $\\mathbf{x}, \\mathbf{y} \\in V$:\n",
    "$$\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\bigg\\langle \\sum_{i=1}^n \\psi_i \\mathbf{b}_i, \\sum_{j=1}^n\\lambda_j\\mathbf{b}_j \\bigg\\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\psi_i \\langle \\mathbf{b}_i, \\mathbf{b}_j \\rangle \\lambda_j = \\hat{\\mathbf{x}}^\\intercal \\mathbf{A} \\hat{\\mathbf{y}}$$\n",
    "Where $\\psi$ and $\\lambda$ are scalars, $A_{i,j} \\coloneqq \\langle \\mathbf{b}_i, \\mathbf{b}_j \\rangle$, and $\\hat{\\mathbf{x}}$ and $\\hat{\\mathbf{y}}$ are the *coordinates* of $\\mathbf{x}$ and $\\mathbf{y}$ with respect to the basis $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the inner product $\\langle \\cdot, \\cdot \\rangle$ is *uniquely determined* by the matrix $\\mathbf{A}$. Furthermore, $\\mathbf{A}$ is a symmetric positive definite matrix, by the definition of an inner product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3.4 - Symmetric, Positive Definite Matrix:**\n",
    "- A matrix $\\mathbf{A}$ is symmetric if:\n",
    "$$\\mathbf{A} = \\mathbf{A}^\\intercal$$\n",
    "- A matrix $\\mathbf{A}$ is positive definite if:\n",
    "$$\\forall \\mathbf{x} \\in V \\setminus \\{\\mathbf{0}\\} : \\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} > 0$$\n",
    "- A matrix $\\mathbf{A}$ is symmetric positive definite if it is both symmetric and positive definite\n",
    "    - A matrix $\\mathbf{A}$ is **positive semidefinite** if only $\\ge$ holds in the condition for positive definiteness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathbf{A} \\in \\reals^{n \\times n}$ is symmetric, positive definite, then:\n",
    "$$\\langle \\bf x, y \\rangle = \\hat{x}^\\intercal A \\hat{y}$$\n",
    "*defines* an inner product w.r.t. an ordered basis $B$, where $\\bf \\hat{x}$ and $\\bf \\hat{y}$ are the coordinate representations of $\\mathbf{x}, \\mathbf{y} \\in V$ w.r.t. $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 3.4**:\n",
    "For a vector space $V \\in \\reals$ and an ordered basis $B$ of $V$, $\\langle \\cdot, \\cdot \\rangle : V \\times V \\rightarrow \\reals$ is an *inner product* if and only if there exists a symmetric, positive definite matrix $\\mathbf{A} \\in \\reals^{n\\times n}$ with:\n",
    "$$\\begin{equation} \\langle \\bf x, y \\rangle = \\hat{x}^\\intercal A \\hat{y} \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of a symmetric and positive definite matrix $\\mathbf{A} \\in \\reals^{n \\times n}$:\n",
    "- The null space of $\\bf A$ consists *only* of $\\bf 0$\n",
    "    - This is because $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} > 0, \\ \\forall \\bf x \\ne 0$\n",
    "    - This implies that $\\bf Ax \\ne 0, \\ \\forall x \\ne 0$\n",
    "    - So, we cannot lose information by applying the transform $\\bf A$\n",
    "- The diagonal elements $a_{ii}$ of $\\bf A$ are positive\n",
    "    - This follows from the positive definite property of $\\bf A$, specifically the fact that $a_{ii} = \\mathbf{e}_i^\\intercal \\mathbf{A} \\mathbf{e}_i > 0$\n",
    "        - Where $\\mathbf{e}_i$ is the $i^{th}$ standard basis vector in $\\reals^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Lengths and Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner products and norms are closely related. Inner products *induce* norms. They map vectors to a real number such that the properties of norms hold. Specifically:\n",
    "$$\\|\\bf x \\| \\coloneqq \\sqrt{\\langle x, x \\rangle}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, *not every norm* is induced by an inner product. E.g., the $l_1$ or \"Manhatten\" norm does not have a corresponding inner product. \n",
    "\n",
    "For an inner product vector space $(V, \\langle \\cdot, \\cdot \\rangle)$ the induced norm $\\|\\cdot\\|$ satisfies the ***Cauchy-Shwarz Inequality***:\n",
    "$$\\bf |\\langle x, y \\rangle| \\le \\|x\\|\\|y\\|$$\n",
    "Written out: the absolute value of the inner product of $\\mathbf{x}, \\mathbf{y} \\in V$ is at most the product of the two vectors' norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norm of a vector induced by one inner product *need not be equal* to the norm of the same vector induced by a different inner product (see example 3.5). However, by the Cauchy-Shwarz inequality, all inner products must be at most the squared norm of the vector. This means that the *length* of a vector may be different depending on which inner product we use to evaluate it. This is not ground-breaking, since we already have two notions of length ($l_1$ vs. $l_2$) which evaluate to different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3.6**: Distance and Metric\\\n",
    "Consider an inner product vector space $(V, \\langle \\cdot, \\cdot \\rangle)$:\n",
    "$$d(\\bf x, y) \\coloneqq \\sqrt{\\langle x - y, x - y \\rangle}$$\n",
    "$d(\\bf x, y )$ is the ***Distance*** between $\\bf x$ and $\\bf y$ for all $\\mathbf{x}, \\mathbf{y} \\in V$. The Euclidean distance is given when the inner product is the dot-product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\\\n",
    "We may compute the distance between vectors without an inner product, having a norm of the two vectors is sufficient. E.g. the Manhatten distance relies on the $l_1$ norm and is given by: \n",
    "$$d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1 = \\sum_{n=1}^N |x_i - y_i |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *mapping* from $\\bf x, y$ to $d(\\bf x, y)$ is called a ***Metric***:\n",
    "$$d : V \\times V \\rightarrow \\reals \\\\ (\\mathbf{x}, \\mathbf{y}) \\mapsto d(\\bf x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric $d$ satisfies the following properties:\n",
    "1. $d$ is positive definite\n",
    "    - $d(\\mathbf{x}, \\mathbf{y}) \\ge 0, \\ \\forall \\mathbf{x}, \\mathbf{y} \\in V$\n",
    "    - $d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}$\n",
    "2. $d$ is symmetric\n",
    "    - $d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x}), \\ \\forall \\mathbf{x}, \\mathbf{y} \\in V$\n",
    "3. Adheres to the Triangel Inequality\n",
    "    - $d(\\mathbf{x}, \\mathbf{z}) \\le d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z}), \\ \\forall \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\\\n",
    "The properties of inner products and of metrics are very similar. However, note that inner products and metrics have opposite behaviors. Specifically, vectors that are similar have larger inner products while they have smaller metrics. Notably, $\\langle \\bf x, y \\rangle$ increases as $\\bf x$ approaches $\\bf y$ while $d(\\bf x, y)$ decreases towards $0$ as $\\bf x$ approaches $\\bf y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Angles and Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may define the angle between two vectors within an inner-product space using the Cauchy-Shwarz inequality:\n",
    "$$-1 \\le \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\| \\mathbf{y} \\|} \\le 1$$\n",
    "From the unit-circle, there exists a unique $\\omega \\in [0, \\pi]$ such that:\n",
    "$$\\cos\\omega = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\| \\mathbf{y} \\|}$$\n",
    "Or, equivalenetly:\n",
    "$$\\cos\\omega = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\sqrt{\\langle \\bf x, x \\rangle \\langle y, y \\rangle}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This unique $\\omega$ is the *angle* between the vectors $\\bf x$ and $\\bf y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3.7** Orthogonality:\\\n",
    "Two vectors $\\bf x$ and $\\bf y$ are ***Orthogonal*** if and only if $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0$, denoted $\\bf x \\perp y$. If the vectors are *unit vectors* (i.e. their magnitudes are both $1$) then they are ***Orthonormal***\n",
    "- It follows that the vector $0$ is orthogonal to *every vector* in a vector space.\n",
    "- We can see that orthogonal vectors are perpendicular from the definition of vecotr angles above:\n",
    "$$\\cos 90\\degree = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\\\n",
    "Vectors that are orthogonal w.r.t. one inner product, *may not be orthogonal* w.r.t a different inner product. When dealing with real-valued spaces, we typically deal with dot-products as inner products. However, we may want to use a different inner product for some purpose, in which case we may need to be cautious about assuming or evaluating orthogonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3.8** Orthogonal Matrix:\\\n",
    "A square matrix $\\mathbf{A} \\in \\reals^{n \\times n}$ is an orthogonal matrix if and only if its columns are orthonormal so that:\n",
    "$$\\bf AA^\\intercal = I = A^\\intercal A$$\n",
    "This implies that $\\bf A^{-1} = A^\\intercal$\\\n",
    "Note that these matrices may be more precisely called \"orthonormal matrices\".\\\n",
    "Transformations with orthogonal matrices preseve distances and angles. So, a vector $\\bf x$ is not changed when transforming it using an orthogonal matrix $\\bf A$:\n",
    "$$\\| \\bf A x \\|_2^2 = (Ax)^\\intercal(Ax) = x^\\intercal A^\\intercal Ax = x^\\intercal Ix = x^\\intercal x = \\|x\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, transformations like rotations and flips are orthogonal matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Orthonormal Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orthonormal basis is a special set of basis vectors in whic each vector is a unit vector, and all vectors are *orthogonal* to each other.\n",
    "\n",
    "**Definition 3.9** Orthonormal Basis:\\\n",
    "A basis $\\{\\mathbf{b}_1, ..., \\mathbf{b}_n\\}$ of $V\\in\\reals^n$ is *orthonormal* if:\n",
    "$$\\langle \\mathbf{b}_i, \\mathbf{b}_j \\rangle = 0, \\ \\forall i \\ne j \\\\ \\langle \\mathbf{b}_i, \\mathbf{b}_i \\rangle = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Orthogonal Complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector spaces may also be orthogonal to each other. Consider $V\\in\\reals^n$ and $U\\in\\reals^m$ where $U \\subseteq V$. The ***Orthogonal Complement*** $U^\\perp$ is an $(N-M)$-dimensional subspace of $V$ and contains all vectors in $V$ that are orthogonal to every vector in $U$. Furthermore, $U\\cap U^\\perp = \\{\\mathbf{0}\\}$ so any vector $\\mathbf{x} \\in V$ can be uniquely decomposed into:\n",
    "$$\\mathbf{x} = \\sum_{m=1}^M \\lambda_m\\mathbf{b}_m + \\sum_{j=1}^{N-M} \\psi_j \\mathbf{b}_j^\\perp, \\ \\ \\lambda_m, \\psi_j \\in \\reals$$\n",
    "Where $(\\mathbf{b}_1, ..., \\mathbf{b}_M)$ and $(\\mathbf{b}_1^\\perp, ..., \\mathbf{b}_{N-M}^\\perp)$ are bases of $U$ and $U^\\perp$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Inner Product of Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inner product of two functions $u: \\reals \\rightarrow \\reals$ and $v: \\reals \\rightarrow \\reals$ can be defined as a *definite integral*:\n",
    "$$\\langle u, v \\rangle \\coloneqq \\int_a^b u(x)v(x)dx$$\n",
    "- Orthogonality:\n",
    "    - if $\\langle u, \\rangle v = 0$, then $u$ and $v$ are *orthogonal functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Orthogonal Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3.10** Projection:\\\n",
    "Let $V$ and $U$ be vector spaces with $U\\subseteq V$. A linear mapping $\\pi:V\\rightarrow U$ is a *projection* if: $$\\pi^2 = \\pi \\circ \\pi = \\pi$$\n",
    "A *projection matrix* $\\mathbf{P}_\\pi$ is then a special kind of transformation matrix that exhibits the property: $$\\mathbf{P}_\\pi^2 = \\mathbf{P}_\\pi$$\n",
    "\n",
    "Intuitively, we may read this as the effect of applying a projection twice is the same as applying it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection onto One-Dimensional Subspaces (Lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a line through the origin with a basis vector $\\mathbf{b} \\in \\reals^n$. The line is a one-dimensional subspace $U \\subseteq \\reals^n$ spanned by $\\bf b$. The orthogonal projection of a vector $\\mathbf{x}\\in \\reals^n$ onto this line $U$ is $\\pi_U(\\mathbf{x}) \\in U$ that is *closest* to $\\bf x$.\n",
    "- \"*Closest*\" means that the distance $\\|\\mathbf{x} - \\pi_U(\\mathbf{x})\\|$ is minimized. This minimization occurs at the point (vector) on $U$ that is orthogonal to $\\bf x$, or more precisely, the line segment $\\pi_U(\\mathbf{x}) - \\bf x$ is orthogonal to $U$.\n",
    "- The projection $\\pi_U(\\bf{x})$ of $\\bf x$ onto $U$ *must be* an element of $U$ and, therefore, a multiple of $\\bf b$, so:\n",
    "$$\\pi_U(\\mathbf{x}) = \\lambda \\bf b, \\ \\lambda \\in \\reals$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding the coordinate $\\lambda$\n",
    "    $$\\text{From orthogonality: } \\ \\langle \\mathbf{x} - \\lambda \\mathbf{b}, \\mathbf{b} \\rangle = 0$$\n",
    "    $$\\text{From bilinearity of the inner product: } \\ \\langle\\mathbf{x}, \\mathbf{b} \\rangle - \\lambda \\langle\\mathbf{b}, \\mathbf{b}\\rangle = 0 \\iff \\lambda = \\bf \\frac{\\langle b, x \\rangle}{\\|b\\|_2^2}$$\n",
    "    $$\\implies \\lambda = \\bf \\frac{b^\\intercal x}{b^\\intercal b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finding the projection point $\\pi_U(\\mathbf{x}) \\in U$\n",
    "$$\\pi_U(\\mathbf{x}) = \\lambda \\bf b = \\frac{b^\\intercal x}{\\|b\\|_2^2}b$$\n",
    "It follows that the length of the projection is:\n",
    "$$\\|\\pi_U(\\mathbf{x})\\| = \\|\\lambda\\mathbf{b}\\| = |\\lambda|\\|\\bf b\\|$$\n",
    "Which, when the inner product is the dot product, may be expressed as:\n",
    "$$\\|\\pi_U(\\mathbf{x})\\| = |\\cos \\omega|\\|\\bf x\\|$$\n",
    "By the definition of the angle between vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finding the projection matrix $\\mathbf{P}_\\pi$\\\n",
    "Because $\\mathbf{P}_\\pi$ is a linear mapping, $\\pi_U(\\mathbf{x}) = \\mathbf{P}_\\pi\\bf x$:\n",
    "$$\\pi_U(\\mathbf{x}) = \\lambda \\bf b = \\frac{bb^\\intercal}{\\|b\\|_2^2}x$$\n",
    "$$\\implies \\mathbf{P}_\\pi = \\frac{bb^\\intercal}{\\|b\\|_2^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\\\n",
    "The projection $\\pi_U(\\mathbf{x}) \\in \\reals^n$ is still an $n$-dimensional vector and not a scalar. However, we no longer require $n$ coordinates to represent the projection. We only need *a single* coordinate to express it with respect to the basis vector $\\bf b$ that spans $U$. This coordinate is $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\\\n",
    "It can be shown (and supposedly is explored in ch 4) that $\\pi_U(\\mathbf{x})$ is an *eigenvector* of $\\mathbf{P}_\\pi$ with and eigenvalue of $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection onto General Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
