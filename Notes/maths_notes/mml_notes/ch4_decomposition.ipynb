{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18827d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c60388",
   "metadata": {},
   "source": [
    "# 4.1 Determinant and Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b042a9e",
   "metadata": {},
   "source": [
    "Determinants are only defined for square matrices, and so are only relevant for linear operators.\\\n",
    "The determinant is denoted $\\det A$ or $|A|$ for square matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d6e87",
   "metadata": {},
   "source": [
    "Determinants map linear operators to scalar values on $\\reals$. Loosely, they capture how the volume of a subspace of the domain of an operator changes when transformed by that operator. This is conceptually why determinants are only defined for linear operators, because the range must be a subspace of the domain. Additionally, this motivates the next theorem which is a handy criterion for determining whether an operator is invertible:\n",
    "\n",
    "**Theorem 4.1: A square matrix is invertible if and only if its determinant is non-zero**\\\n",
    "For a square matrix $A \\in \\reals^{n\\times n}$: $$A \\text{ is invertible } \\iff \\det A \\ne 0$$\n",
    "For linear operator $T$, this directly implies: $$\\text{null } T = \\{0\\} \\iff \\det \\mathcal{M}(T) \\ne 0$$\n",
    "\n",
    "This is intuitive using the concept of volume since a determinant of 0 would correspond with a scaling of some subset of the vector space to null, implying that the operator is non-injective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eade5a",
   "metadata": {},
   "source": [
    "**Determinants of Triangular Matrices:**\\\n",
    "For a triangular matrix (upper or lower) $A$, the determinant of $A$ is: $$\\det A = \\prod_{i=1}^n A_{ii}$$\n",
    "That is, the determinant is the product of its diagonal elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1f166",
   "metadata": {},
   "source": [
    "**Determinants Measure Volume**\\\n",
    "For square matrix $A$ with elements in $\\reals^n$, the determinant $\\det A$ is the signed volume of an $n$-dimensional parallelepiped formed by the columns of the matrix $A$. (i.e. each column vector is an edge of the parallelepiped)\n",
    "- The sign of the determinant indicates the orientation of the spanning vectors w.r.t. an orthonormal basis of the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee955c",
   "metadata": {},
   "source": [
    "**Theorem 4.2: Laplace Expansion**\\\n",
    "Consider a matrix $A\\in\\reals^{n\\times n}$:\n",
    "1. Expansion along column $j$\n",
    "$$\\det A = \\sum_{k=1}^n (-1)^{k+j} a_{kj} \\det A_{k,j}$$\n",
    "2. Expansion along row $j$\n",
    "$$\\det A = \\sum_{k=1}^n (-1)^{k+j} a_{jk} \\det A_{j,k}$$\n",
    "\n",
    "Where $A_{k,j} \\in\\reals^{(n-1)\\times(n-1)}$ is the submatrix of $A$ obtained by deleting row $k$ and column $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c36af6",
   "metadata": {},
   "source": [
    "This theorem provides an algorithm through which we may find the determinant of any square matrix by recursively computing determinants of submatrices. Specifically, we will recurr down to the $2\\times 2$ submatrices, then sum up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3b86e",
   "metadata": {},
   "source": [
    "**Properties of Determinants**:\n",
    "- $\\det AB = \\det A \\big(\\det B\\big)$\n",
    "- $\\det A = \\det A^\\intercal$\n",
    "- For invertible $A$, $\\det A^{-1} = 1/ (\\det A)$\n",
    "- The determinant is invariant to the choice of basis\n",
    "- Elementary row operations do not change the determinant \n",
    "    - Except for scalar multiplication: Scaling a column/row scales the determinant, with $\\det (\\lambda A) = \\lambda^n \\det A$\n",
    "- Swapping rows/columns changes the sign of $\\det A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf7843",
   "metadata": {},
   "source": [
    "**Theorem 4.3: A square matrix has a non-zero determinant if and only if it is full rank**\\\n",
    "This directly follows from the fact that linear operators are invertible if and only if they are bijective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf149d",
   "metadata": {},
   "source": [
    "**Definition 4.4: Trace**\\\n",
    "The trace of a square matrix $A\\in\\reals^{n\\times n}$ is: $$\\text{tr} A \\coloneqq \\sum_{i=1}^n a_{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279dc10",
   "metadata": {},
   "source": [
    "**Properties of the Trace:**\n",
    "- $\\text{tr}\\big(A + B\\big) = \\text{tr}A + \\text{tr}B$, for $A,B\\in\\reals^{n\\times n}$\n",
    "- $\\text{tr}(\\alpha A) = \\alpha \\text{tr}A$, for $\\alpha\\in\\reals, \\ A\\in\\reals^{n\\times n}$\n",
    "- $\\text{tr}I_n = n$\n",
    "- $\\text{tr}(AB) = \\text{tr}(BA)$, for $A\\in\\reals^{n,k}, B\\in\\reals^{k,n}$\n",
    "- The trace is invariant to the choice of basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dced659",
   "metadata": {},
   "source": [
    "**Definition 4.5: Characteristic Polynomial**\\\n",
    "For $A\\in\\reals^{n,n}, \\ \\lambda \\in \\reals$: \n",
    "$$p_A(\\lambda) \\coloneqq \\det(A - \\lambda I) = c_0 + c_1\\lambda + \\cdots + c_{n-1}\\lambda^{n-1} + (-1)^n\\lambda^n$$\n",
    "Where, $$c_0 = \\det A \\\\ \\ \\\\ c_{n-1} = (-1)^{n-1}\\text{tr}A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a7105",
   "metadata": {},
   "source": [
    "This is a monic polynomial of order $n$. This is different from the minimial polynomial of an operator since is defines a scalar and since it is of order $n$ (where the minimal polynomial may have a lower degree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987580e",
   "metadata": {},
   "source": [
    "\n",
    "# 4.2 Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02af2fb",
   "metadata": {},
   "source": [
    "Eigens of a matrix:\n",
    "$$Av = \\lambda v, \\\\ \\ \\\\ A\\in\\reals^{n,n}, \\ v\\in\\reals^n\\setminus\\{0\\}, \\ \\lambda\\in\\reals$$\n",
    "Here, $\\lambda$ is an eigenvalue and $v$ is its associated eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2e2ae",
   "metadata": {},
   "source": [
    "**Properties of Eigenvalues**\\\n",
    "The following are equivalent:\n",
    "- $\\lambda$ is an eigenvalue of $A\\in\\reals^{n,n}$\n",
    "- $\\exists v \\in \\reals^n\\setminus\\{0\\} \\ : \\ Av = \\lambda v$\n",
    "- $(A - \\lambda I)v = 0$ is non-trivial\n",
    "- $\\text{rk}(A - \\lambda I) < n$\n",
    "- $\\det(A - \\lambda I) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fb5ee",
   "metadata": {},
   "source": [
    "**Definition 4.7: Collinearity and Codirection**\\\n",
    "Two vectors that point in the same direction are collinear. Two vectors that point in the same or opposite directions are codirected.\n",
    "\n",
    "Eigenvectors are not unique. Any vector that is collinear with an eigenvector $v$ is also an eigenvector of $A$ associated with the same eigenvalue $\\lambda$ as $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c4dcd",
   "metadata": {},
   "source": [
    "**Theorem 4.8: Eigenvalues are roots of the characteristic polynomial**\\\n",
    "For matrix $A\\in \\reals^{n,n}$: $$\\lambda \\text{ is an eigenvalue of } A \\iff \\lambda \\text{ is a root of } p_A(\\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62070de5",
   "metadata": {},
   "source": [
    "**Definition 4.9: Algebraic Multiplicity**\\\n",
    "The algebraic multiplicity of an eigenvalue $\\lambda$ is the number of times that it appears as a root of the characteristic polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8f74d",
   "metadata": {},
   "source": [
    "**Definition 4.10: Eigenspace and Eigenspectrum**\\\n",
    "For $A\\in\\reals^{n,n}$, an *eigenspace* $E_\\lambda$ is the subspace of $\\reals^n$ spanned by the eigenvectors associated with each eigenvalue of $A$, while the eigenspectrum (or spectrum) is the set of all eigenvalues of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823986df",
   "metadata": {},
   "source": [
    "**Useful Properties of Eigens:**\n",
    "- $A$ and $A^\\intercal$ have the same eigenvalues, but not necessarily the same eigenvectors\n",
    "- $E_\\lambda = \\text{null } (A - \\lambda I)$\n",
    "- Eigenvalues are invariant to the choice of basis (like determinants and traces)\n",
    "- Symmetric, positive definite matrices always have real, positive eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70889b",
   "metadata": {},
   "source": [
    "**Definition 4.11: Geometric Multiplicity**\\\n",
    "For eigenvalue $\\lambda$, the geometric multiplicity of $\\lambda$ is the number of linearly independent eiegnevectors assoiated with $\\lambda$\n",
    "- Equivalently, it is the dimensionality of the eigenspace $E_\\lambda$\n",
    "- Geometric multiplicity is always less than or equal to algebraic multiplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700da634",
   "metadata": {},
   "source": [
    "**Theorem 4.12: Eigenvectors with Distinct Eigenvalues are Linearly Independent**\\\n",
    "Eigenvectors $v_1,...,v_n$ associated with distinct eigenvalues $\\lambda_1,...,\\lambda_n$ are linearly independent.\n",
    "- Thus, if we have $n$ distinct eigenvalues in $\\reals^n$ then the $n$ associated eigenvectors form a basis for $\\reals^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fa643",
   "metadata": {},
   "source": [
    "**Definition 4.13: Defective**\\\n",
    "A square matrix is *defective* if it possesses fewer than $n$ linearly independent eigenvectors\n",
    "- This implies that a defective matrix cannot have $n$ distinct eigenvalues\n",
    "\n",
    "**NOTE:** Defective matrices may still be full rank, e.g. $A = [(1, 1), (0, 1)]$ which is a full-rank upper-triangular matrix with only one independent eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbd398",
   "metadata": {},
   "source": [
    "**Theorem 4.14: We can always obtain a symmetric matrix from another matrix**\\\n",
    "Given $A\\in\\reals^{m,n}$, we may obtain a symmetric positive-semidefinite matrix $S$ via: $$S\\coloneqq A^\\intercal A$$\n",
    "When $\\text{rk}(A) = n$, then $S$ is symmetric positive-definite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05ba25",
   "metadata": {},
   "source": [
    "**Theorem 4.15: Spectral Theorem**\\\n",
    "For $A\\in\\reals^{n,n}$, if $A$ is symmetric, then there exists an orthonormal basis of $\\reals^n$ consisting of eigenvectors of $A$ with *real* eigenvalues\n",
    "\n",
    "This implies that any symmetric matrix in a real vector space may be decomposed into: $$A = PDP^\\intercal$$ Where $D$ is a diagonal matrix and $P$ is a matrix with column-vectors of the eigenvectors of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1302c9",
   "metadata": {},
   "source": [
    "**Theorem 4.16: Determinant Equals Product of Eigenvalues**\\\n",
    "For $A\\in\\reals^{n,n}$: $$\\det A = \\prod_{i=1}^n \\lambda_i$$\n",
    "Where $\\lambda_i\\in\\mathbb{C}$ are eigenvalues of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221705c2",
   "metadata": {},
   "source": [
    "**Theorem 4.17: Trace is the Sum of Eigenvalues**\\\n",
    "For $A\\in\\reals^{n,n}$: $$\\text{tr}(A) = \\sum_{i=1}^n \\lambda_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabde751",
   "metadata": {},
   "source": [
    "# 4.3 Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b3d36",
   "metadata": {},
   "source": [
    "**Theorem 4.18: Cholesky Decomposition**\\\n",
    "A *symmetric positive-definite* matrix $A$ can be factorized into $A=LL^\\intercal$ where $L$ is a lower-triangular matrix with posistive diagonal elements.\n",
    "- $L$ is called the \"Cholesky Factor\" of $A$\n",
    "- $L$ is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d10d5",
   "metadata": {},
   "source": [
    "**NOTE:** LADR Ch 7 covers QR-factorization and Cholesky Decomposition. QR factorization decomposes positive square matrices $A$ into a unitary matrix $Q$ and upper-triangular matrix $R$ through Gram-Schmidt orthogonalization. Thus, it is an algorithmically easy decomposition. The Cholesky Decomposition may be yielded from the QR factorization directly as:\n",
    "$$A = R^\\intercal Q^\\intercal QR = R^\\intercal R = LL^\\intercal$$\n",
    "Where $Q^\\intercal Q = I$ because $Q$ is unitary (follows from definition of unitary).\\\n",
    "Thus the Cholesky Decomposition may be quickly computed using Gram-Schmidt orthogonalization via the QR-factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68555d2",
   "metadata": {},
   "source": [
    "# 4.4 Eigendecompoistion and Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b557c",
   "metadata": {},
   "source": [
    "Diagonal matrices are matrices of all zero elements except possibly along the diagonal.\\\n",
    "The diagonal elements of diagonal matrices are eigenvalues of the matrix. Thus, the determinant of a diagonal matrix is simply the product of its diagonal.\\\n",
    "Correspondingly, the inverse of a diagonal matrix $D$ is simply the diagonal matrix $D^{-1}$ with reciprocal diagonal elemtns. This follows from the expression for the determinant of an inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28e1b1",
   "metadata": {},
   "source": [
    "**Definition 4.19: Diagonalizable**\\\n",
    "A square matrix $A\\in\\reals^{n,n}$ is diagonalizable if it is *similar* to a diagonal matrix. That is, if there exists and invertible matrix $P\\in\\reals^{n,n}$ such that: $$D = P^{-1}AP$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b8b42",
   "metadata": {},
   "source": [
    "This is effectively a change in basis, corresponding to the definition in LADR that an operator is diagonalizable if it has a diagonal matrix w.r.t. some basis of its space.\\\n",
    "It turns out that an operator is diagonalizable if and only if its space has a basis consisting of its eigenvectors. Equivalently, a square matrix $A$ is diagonalizable if and only if its eigenvectors form a basis.\\\n",
    "This implies that any square matrix $A\\in\\reals^{n,n}$ with $n$ distinct eigenvalues is diagonalizable (although it *may be* diagonalizable with fewer than $n$ distinct eigenvalues as well).\n",
    "\n",
    "We may observe that the diagonal matrix is composed of eigenvalues of $A$ by noting:\n",
    "$$AP = PD \\implies Ap_1 + \\cdots + Ap_n = \\lambda_1p_1 + \\cdots + \\lambda_n p_n$$\n",
    "Implying that, $$Ap_1 = \\lambda_1p_1 \\cdots Ap_n = \\lambda_np_n$$\n",
    "\n",
    "These observations about eigenvalues leads to the next theorem statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d2772",
   "metadata": {},
   "source": [
    "**Theorem 4.20: Eigendecomposition**\\\n",
    "A square matrix $A\\in\\reals^{n,n}$ can be factored into a matrices $P,D\\in\\reals^{n,n}$ where $D$ is a diagonal matrix of eigenvalues of $A$ if and only if the eigenvectors of $A$ form a basis of $\\reals^n$\n",
    "- This implies that only *non-defective* matrices are diagonalizable\n",
    "- This implies that the column-vectors of $P$ are eigenvectors of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872ea1",
   "metadata": {},
   "source": [
    "**Theorem 4.21: Symmetric Matrices are Always Diagonalizable**\\\n",
    "A symmetric matrix $S\\in\\reals^{n,n}$ may always be diagonalized\n",
    "- This follows directly from the spectral theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cac757",
   "metadata": {},
   "source": [
    "# 4.5 Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9521fa8",
   "metadata": {},
   "source": [
    "**Theorem 4.22: SVD Theorem**\\\n",
    "Any matrix $A\\in\\reals^{m,n}$ may be expressed as a composition of orthogonal (i.e. orthonormal) matrices $U\\in\\reals^{m,m}$ and $V\\in\\reals^{n,n}$ and diagonal matrix $\\Sigma\\in\\reals^{m,n}$ with positive real elements, such that:\n",
    "$$A = U\\Sigma V^\\intercal$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beee45f",
   "metadata": {},
   "source": [
    "$U$ is the matrix comprised of eigenvectors of the symmetric matrix $AA^\\intercal$, these vectors are sometimes referred to as \"*left singular vectors*\". \\\n",
    "$V$ is the matrix comprised of eigenvectors of the symmetric matrix $A^\\intercal A$, these vectors are sometimes called the \"*right singular vectors*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa370a",
   "metadata": {},
   "source": [
    "**NOTE:**\\\n",
    "We can use the SVD to get eigendecompositions for $A^\\intercal A$ or $AA^\\intercal$:\n",
    "$$A^\\intercal A = V\\Sigma U^TU\\Sigma V^T = V\\Sigma^2 V^\\intercal$$\n",
    "Where $\\Sigma = \\Sigma^\\intercal$ since $\\Sigma$ is diagonal, and $U^\\intercal U = I$ since $U$ is unitary.\\\n",
    "Likewise: $$AA^\\intercal = U\\Sigma^2 U^\\intercal$$\n",
    "Singular values are the real square roots of the eigenvalues of $A^\\intercal A$. They are *also* the square roots of the eigenvalues of $AA^\\intercal$. However, if $m < n$, then only the first $m$ singular values are eigenvalues of $AA^\\intercal$, and vis-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ba1cc",
   "metadata": {},
   "source": [
    "**NOTE:**\\\n",
    "$U$ and $V$ are unitary matrices. A key property of a unitary matrix like $U$ is: $$U^{-1} = U^\\intercal$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5898921",
   "metadata": {},
   "source": [
    "**NOTE:**\\\n",
    "The matrix $U$ provides an orthonormal basis of the vector space $\\reals^m$ and the matrix $V$ provides an orthonormal basis of the vector space $\\reals^n$ (spectral theorem 4.15 and the eigendecompositions above).\n",
    "\n",
    "From this, we may understand the SVD as akin to a change of basis. Consider $A$ as the matrix representation of a linear operator. That is: \n",
    "$$A = \\mathcal{M}(T), \\\\ T\\in\\mathcal{L}(\\reals^n, \\reals^m)$$\n",
    "Then, for some mapping $Tv = w, \\ v\\in\\reals^n, \\ w\\in\\reals^m$:\n",
    "$$Tv = Av = U\\Sigma V^\\intercal v$$\n",
    "- $V^\\intercal$ takes $v$ to the orthonormal eigenbasis of $\\reals^n$\n",
    "    - Note that because $V$ is orthogonal, the norm of $v$ is preserved (if $V^\\intercal$ were acting on a set of column vectors, then their angles would be preserved as well)\n",
    "- $\\Sigma$ stretches the representation of $v$ is the coordinates of the orthonormal eigenbasis of $\\reals^n$ ***AND*** maps to the codomain of $\\reals^m$\n",
    "- $U$ gives the coordinate representation of the vector given by $\\Sigma V^\\intercal v$ in the orthonormal eigenbasis of $\\reals^m$\n",
    "    - i.e. it does a change of basis in $\\reals^m$\n",
    "\n",
    "So, we basically have a change of basis in the domain $\\reals^n$ by $V^\\intercal$, a scaling and mapping to the codomain $\\reals^m$ by $\\Sigma$, and finally a change of basis in the codomain $\\reals^m$ by $U$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867a234",
   "metadata": {},
   "source": [
    "# 4.6 Matrix Approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355bb79",
   "metadata": {},
   "source": [
    "**Rank-$k$ Approximation:**\\\n",
    "A rank-$k$ approximation is comprised of the first $k$ singular values and the rank-1 matrices formed by the the outer product of the left and right singular vectors:\n",
    "$$\\hat{A} (k) \\coloneqq \\sum_{i=1}^k \\sigma_i u_iv_i^\\intercal = \\sum_{i=1}^k \\sigma_i A_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56c0f0",
   "metadata": {},
   "source": [
    "This enables us to form a low-rank approximation of the orginal matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f17a21",
   "metadata": {},
   "source": [
    "**Definition 4.23: Spectral Norm of a Matrix**\\\n",
    "For $A\\in\\reals^{m,n}$ and $v\\in\\reals^n\\setminus\\{0\\}$, the spectral norm of $A$ is: $$\\|A\\|_2 \\coloneqq \\max_v \\frac{\\|Av\\|_2}{\\|v\\|_2}$$\n",
    "Where the subscript of $2$ denotes the Euclidean ($l2$) norm.\\\n",
    "The spectral norm determines the maximum length any vector $v$ may become when transformed by $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f252cf4",
   "metadata": {},
   "source": [
    "**Theorem 4.24: The Spectral Norm is the Largest Singular Value**\\\n",
    "The spectral norm of $A$ is equal to its largest singular value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37851225",
   "metadata": {},
   "source": [
    "**Theorem 4.25: Eckart-Young Theorem**\\\n",
    "For matrices $A\\in\\reals^{m,n}$ of rank $r$ and $B\\in\\reals^{m,n}$ of rank $k$, for any $k\\le r$ with $\\hat{A}(k) = \\sum\\sigma_i u_i v_i^\\intercal$:\n",
    "$$\\hat{A}(k) = \\argmin_{\\text{rk}(B)=k} \\|A - B\\|, \\\\ \\ \\\\ \\|A - \\hat{A}(k)\\| = \\sigma_{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640380d",
   "metadata": {},
   "source": [
    "This theorems states that the rank-$k$ approximation $\\hat{A}(k)$ is the matrix nearest to $A$ in Euclidean distance of *any* rank-$k$ matrix in $\\reals^{m,n}$, and that this minimum distance is $\\sigma_{k+1}$\\\n",
    "This follows from theorem 4.24, since the singular value $\\sigma_{k+1}$ is the largest singular value of the difference matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69319ca3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
