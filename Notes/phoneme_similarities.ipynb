{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9841fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abbyv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\abbyv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from metaphone import doublemetaphone\n",
    "import scipy.stats as stats\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfbc1d",
   "metadata": {},
   "source": [
    "### Some Rambling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a60645",
   "metadata": {},
   "source": [
    "After much trial and error with simpler approaches like clustering and metaphones, I'm resolving to use pretrained phonetic embeddings.\\\n",
    "This team https://github.com/rahulsrma26/phonetic-word-embedding published a pretrained set of embeddings for the `cmudict` english phonetic dictionary.\\\n",
    "A scan of their paper suggests their basic approach was:\n",
    "- Encode phonemes into Arpabet articulatory features -- (using phoneme spellings from the `cmudict`)\n",
    "- Compute a custom Jaccard similarity score for bigrams of phoneme features that weights terminating vowel phonemes more highly (e.g. \"marrY\")\n",
    "- Chain these bigram similarities into word similarity sums\n",
    "- Compute a similarity matrix $M$ by computing this simliarity across all pairs of words\n",
    "- Use SGD to learn word embeddings $V$ by minimizing loss $||M - VV^\\intercal||^2$\n",
    "\n",
    "**ABOUT THE LOSS**:\\\n",
    "This is pretty neat. For a dictionary of $d$ words and an embedding space of $m$ dimensions, $M \\in \\reals^{d\\times d}$ and $V \\in \\reals^{d\\times m}$. Thus, $VV^\\intercal$ is actually a ***factorization*** of $M$. It gives us $m$-dimensional representations of each word that *preserve* the similarities we defined and computed in $M$. Indeed, $V$ could be derived with matrix-factorization since $M$ is positive symmetric. The authors use SGD instead since $M$ is prohibitively large for factorization.\\\n",
    "It is important to note as well that because we are effectively learning a factorization of the similarity matrix, the choice of the similarity scores used to comprise the matrix is *the most crucial factor* in making these embeddings meaningful. Not the loss function, or the model used for SGD.\n",
    "\n",
    "I think using these embeddings locks my into the words in `cmudict`. While they provide code for their SGD implementation, I don't think I can learn new word embeddings via fine-tuning because I would need to recompute a new similarity matrix $M$ that includes those new words....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb26478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pretrained embeddings from: https://github.com/rahulsrma26/phonetic-word-embedding\n",
    "# this team made an algorithm to compute similarities between the phonetic spelling of words from cmudict (M)\n",
    "# then learnt embeddings for each word (V) by minimimizing ||M - V V'||^2 with SGD\n",
    "# due to the nature of this loss function, I don't think we can add new \n",
    "texts = []\n",
    "with open(r\"C:\\Users\\abbyv\\scripts\\phonetic-word-embedding\\vector_embeddings\\simvecs\", 'r') as file:\n",
    "    for line in file:\n",
    "        texts.append(line)    \n",
    "\n",
    "words = np.array([texts[i].split()[0].lower() for i in range(len(texts))])\n",
    "emb = np.array([texts[i].split()[1:] for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f78dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWrapper:    \n",
    "    mdf = get_mdf()\n",
    "    meta_list = pd.unique(mdf['metaphone'])\n",
    "    ctoi = set(list(''.join(meta_list)))\n",
    "    ctoi = {c : (i * 10) + 1 for i,c in enumerate(ctoi)}  # encode numerically w/ monotonic integers\n",
    "    itoc = {i:c for c,i in ctoi.items()}\n",
    "\n",
    "class MetaMatcher(WordWrapper):\n",
    "    \"\"\"\n",
    "    Find phonetically similar words to provided word based on cosine-similarity to metaphone encodings\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def one_hot(cls, c: str):\n",
    "        return np.array([1 if char == c else 0 for char in cls.ctoi.keys()])\n",
    "    \n",
    "    @classmethod\n",
    "    # def enc(cls, meta: str):\n",
    "    #     res = np.array(list(meta)).reshape(-1, 1)\n",
    "    #     res = np.apply_along_axis(lambda x: one_hot(x), axis=1, arr=res)\n",
    "    #     res = np.pad(res, ((0, 10 - res.shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "    #     return np.ravel(res)\n",
    "    def enc(cls, meta): \n",
    "        res = np.array([cls.ctoi[c] for c in list(meta)])\n",
    "        return np.pad(res, (0, 10 - len(res)), 'constant', constant_values=0)\n",
    "\n",
    "    @classmethod\n",
    "    def dec(cls, vector): \n",
    "        return ''.join([cls.itoc[i] for i in vector])\n",
    "\n",
    "    @classmethod\n",
    "    def vectorize(cls, meta_list):\n",
    "        \"\"\"Vectorize metaphones within each group\"\"\"\n",
    "        res = np.array([cls.enc(m) for m in meta_list])\n",
    "        return res\n",
    "\n",
    "    def __init__(self, n: int = 10):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.M = self.vectorize(self.meta_list)\n",
    "\n",
    "    def get_similarities(self, meta: str):\n",
    "        v = self.enc(meta)\n",
    "        M_norms = np.apply_along_axis(lambda x: np.linalg.norm(x), axis=1, arr=self.M)\n",
    "        return (v @ self.M.T) / (np.linalg.norm(v) * M_norms)\n",
    "\n",
    "    def sample_similar(self, meta: str, n: int = 1):\n",
    "        \"\"\"Randomly sample a similar metaphone from the distribution of cosine similarites\"\"\"\n",
    "        cs = self.get_similarities(meta)              # get cosine similarities\n",
    "        cs *= np.exp(cs)  # try scaling\n",
    "        probs = np.exp(cs) / np.sum(np.exp(cs))       # softmax over cosine-similarities\n",
    "        rv = np.random.multinomial(1, probs, size=n)  # randomly sample n times, 1 at sampled indexes\n",
    "        idx = np.apply_along_axis(lambda x: np.argmax(x), axis=1, arr=rv)  # get indexes of samples\n",
    "\n",
    "        return self.meta_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c0df8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SKNNK', 'AKNTNT', 'KNTRXNJ', 'APTRTL', 'KRTSSMS', 'ARPPNK',\n",
       "       'ALMNTTF', 'AXR', 'STKSR', 'FLNTT'], dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = MetaMatcher(n=10)\n",
    "mm.sample_similar('ARTFRK', n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
