{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Thumbtack Tossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the probability associated with a thumbtack landing point-up after being tossed. From $n$ tosses, the number of times that the tack lands point-up is denoted $y$. The true probability of landing point-up is denoted $\\theta$. In this text, unknown quantities of a model or distribution are called ***parameters***.\n",
    "\n",
    "Consider a sample in which $N=30$ and $y = 16$. Since the outcome is binary, we may reasonably treat $Y$ as a binomially-distributed random variable, realizations of which are specific values $y$. The functional form of $Y$ is then the probability mass function:\n",
    "$$f(y; n, \\theta) = {n \\choose y} \\theta^y (1-\\theta)^{n-y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentis Approach to Thumbtack Tossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed $n$, we may estimate $\\hat \\theta$ with MLE. Intuitively, the approach simply chooses a value of $\\hat \\theta$ which *maximizes* the likelihood of observing the data. \n",
    "$$\\hat \\theta (y) = \\argmax_\\theta L(\\theta; y)$$\n",
    "The likelihood function is a function *of the parameter being estimated* conditional on the observed quantities. For this binomial random variable, it is:\n",
    "$$L(\\theta | n, y) = P(Y = y | \\theta) = {n \\choose y}\\theta^y (1 - \\theta)^{n-y}$$ \n",
    "If we consider each trial in the $n$ observations as an independent Bernoulli random variable $y^*$, then the likelihood is:\n",
    "$$L(\\theta|y^*, n) = \\prod_{i=1}^n \\theta(y^*_i) = \\prod_{i=1}^n \\theta^{y^*_i} (1-\\theta)^{1-y^*_i} = {n \\choose y}\\theta^y (1 - \\theta)^{n-y}$$\n",
    "The log-likelihood is then:\n",
    "$$\\log L(\\theta|n, y) = l(\\theta|n, y) = y\\log(\\theta) + (n-y)\\log(1 - \\theta)$$\n",
    "Note that ${n\\choose y}$ is dropped from the log-likelihood because it is a constant and may be ignored for the pruposes of maximizing w.r.t. the parameter $\\theta$.\n",
    "\n",
    "Thus, it is generally the case that we may simply take the log of the PMF (or PDF for continuous distributions) as the log-likelihood. However, the meaning of the likelihood is different than the meaning of the PMF/PDF. Crucially, the likelihood is a function of the parameter being estimated conditioned upon the observations. Thus, the likelihood takes the observed values as fixed and treats the parameters as variables. Consequences of this are:\n",
    "1. The PMF must sum to 1 across all possible oberserved outcomes\n",
    "2. The likelihood function need not sum to 1 across all parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
