{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f0663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function ugh\n",
    "def load_py(filename):\n",
    "    import requests, importlib\n",
    "    if not filename.endswith(\".py\"): filename += \".py\"\n",
    "    url = f\"https://raw.githubusercontent.com/BenAF002/data_science/refs/heads/main/colab_checkpoints/{filename}\"\n",
    "    code = requests.get(url).text\n",
    "\n",
    "    # write to /content for colab\n",
    "    destination = f\"/content/{filename}\"\n",
    "    with open(destination, \"w\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "    # handle module reloading\n",
    "    module_name = filename[:-3] # Strip the .py extension\n",
    "    if module_name in globals():\n",
    "        importlib.reload(globals()[module_name])\n",
    "    else:\n",
    "        globals()[module_name] = importlib.import_module(module_name)\n",
    "\n",
    "load_py(\"gpt_homebrew\")  # will need to recall each time script is updated/saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dd6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6825649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device='mps'\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fad3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data manager for tinyshakespeare data\n",
    "class ShakeMgr:\n",
    "    def __init__(self, seq_len: int = 32):\n",
    "        if 'tinyshakespeare_data' not in globals():\n",
    "            import urllib.request\n",
    "            url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "            # read text in from url\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                tinyshakespeare_data = response.read().decode('utf-8')\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.text = tinyshakespeare_data\n",
    "        self.tokens = enc.encode(self.text)\n",
    "\n",
    "        # stack token sequences into data buffer for dataloader\n",
    "        buf_len  = len(self.tokens) - (len(self.tokens) % (seq_len + 1))\n",
    "        self.buffer = self.tokens[:buf_len]\n",
    "        self.buffer = torch.tensor(self.buffer).reshape(-1, seq_len + 1)\n",
    "\n",
    "    def train_test_split(self, train_pct: float = 0.8):\n",
    "        idx = np.random.permutation(data_len := len(self.buffer))\n",
    "        train_data = self.buffer[idx[:int(data_len * train_pct)]]\n",
    "        test_data = self.buffer[idx[int(data_len * train_pct):]]\n",
    "\n",
    "        return ShakeDS(train_data), ShakeDS(test_data)  # return train-test split\n",
    "\n",
    "# dataset class for tinyshakespeare data, compatible with pytorch dataloader\n",
    "class ShakeDS(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.data[idx, :-1]  # shifted input sequence\n",
    "        y = self.data[idx, 1:]   # shifted target sequence\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a094e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new training data with maximum batch-size sequences\n",
    "data = ShakeMgr(seq_len=1024)  # max batch-size\n",
    "train, test = data.train_test_split(train_pct = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1e7e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt = gh.GPT.from_pretrained('gpt2', lora_rank=0, lora_alpha=16)\n",
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig())\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c0507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.992279, dt: 1,298.27ms, tok/sec: 6,309.92\n",
      "step 1, loss: 9.610030, dt: 927.51ms, tok/sec: 8,832.28\n",
      "step 2, loss: 9.131944, dt: 926.60ms, tok/sec: 8,840.93\n",
      "step 3, loss: 9.067685, dt: 934.72ms, tok/sec: 8,764.16\n",
      "step 4, loss: 8.617716, dt: 936.50ms, tok/sec: 8,747.47\n",
      "step 5, loss: 8.445677, dt: 937.33ms, tok/sec: 8,739.75\n",
      "step 6, loss: 8.125593, dt: 931.83ms, tok/sec: 8,791.29\n",
      "step 7, loss: 7.868392, dt: 933.69ms, tok/sec: 8,773.79\n",
      "step 8, loss: 7.779512, dt: 932.62ms, tok/sec: 8,783.84\n",
      "step 9, loss: 7.629560, dt: 938.43ms, tok/sec: 8,729.48\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "\n",
    "# try adjusting precision of pytorch float32 operations\n",
    "# setting to 'high' uses TensorFlow32 (TF32) which subtley adjusts dtypes in operations\n",
    "# increases TFLOPS on GPUs with tensorcores (like A100) by ~8X -- may improve other GPUs too\n",
    "# TF32 is available on GPUs after the \"Ampere\" series (of which A100 is one)\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "import time\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    logits, loss = gpt(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789c8d5",
   "metadata": {},
   "source": [
    "Adjusting to use TF32 in matmul operations increased approximate tokens per second from ~15.7k --> ~42.7k.\\\n",
    "This is less than the ~8k improvement suggested by TFLOPS because of memory constraints.\n",
    "\n",
    "We can do better by improving memory efficiency - i.e. using a floating-point format for the parameter data itself, not just the operation (a bit arcane......). The trade off is reduced precision and potentially a reduced range of value expressions. The format BF16 is a good middle-ground. It maintains the range of expression of FP32 (the default for `float32` dtype), but reduces the precision to reduce the bit-cost in memory. \n",
    "\n",
    "[This tutorial](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html) explains the pytorch implementation for adjusting float formats, particularly using `torch.autocast`. This can be used to adust the formats during the forward pass automatically.\\\n",
    "This only converts some datatypes. The implementation used here won't convert the embedding weights. It sounds like the `nn.Linear` layers will be changed which is good since they tend to be the most expensive here.\n",
    "\n",
    "**Again**: this is only possible to do in Ampere GPUs, older GPUs don't support BF16 (I guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae54d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 7.383627, dt: 943.68ms, tok/sec: 8,680.94\n",
      "step 1, loss: 7.230249, dt: 749.67ms, tok/sec: 10,927.51\n",
      "step 2, loss: 7.142229, dt: 746.48ms, tok/sec: 10,974.18\n",
      "step 3, loss: 6.902237, dt: 748.69ms, tok/sec: 10,941.82\n",
      "step 4, loss: 6.789080, dt: 749.39ms, tok/sec: 10,931.58\n",
      "step 5, loss: 6.764679, dt: 751.12ms, tok/sec: 10,906.43\n",
      "step 6, loss: 6.723163, dt: 749.41ms, tok/sec: 10,931.24\n",
      "step 7, loss: 6.539473, dt: 750.65ms, tok/sec: 10,913.22\n",
      "step 8, loss: 6.486143, dt: 751.10ms, tok/sec: 10,906.62\n",
      "step 9, loss: 6.443795, dt: 750.19ms, tok/sec: 10,919.83\n"
     ]
    }
   ],
   "source": [
    "# add BF16 autocast\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafa678",
   "metadata": {},
   "source": [
    "## torch.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb47ca4",
   "metadata": {},
   "source": [
    "This is a *compiler* for pytorch code which can be used improve efficiency a lot.\\\n",
    "[Docs](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
    "\n",
    "Testing out by just calling compile on the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd41b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = torch.compile(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea11924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "W1210 02:15:23.297000 1460 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.983019, dt: 37,981.59ms, tok/sec: 215.68\n",
      "step 1, loss: 9.558714, dt: 336.32ms, tok/sec: 24,357.79\n",
      "step 2, loss: 9.116148, dt: 337.41ms, tok/sec: 24,279.05\n",
      "step 3, loss: 9.207137, dt: 342.12ms, tok/sec: 23,944.77\n",
      "step 4, loss: 8.644671, dt: 343.04ms, tok/sec: 23,880.37\n",
      "step 5, loss: 8.542477, dt: 338.34ms, tok/sec: 24,212.33\n",
      "step 6, loss: 8.274139, dt: 339.65ms, tok/sec: 24,118.90\n",
      "step 7, loss: 8.107955, dt: 342.17ms, tok/sec: 23,941.45\n",
      "step 8, loss: 7.832716, dt: 339.43ms, tok/sec: 24,134.72\n",
      "step 9, loss: 7.663104, dt: 341.04ms, tok/sec: 24,020.36\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "import time\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    # add BF16 autocast\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b9487",
   "metadata": {},
   "source": [
    "So, this added some time in compilation, but it DOUBLED the approximate tokens per second!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f387c",
   "metadata": {},
   "source": [
    "It sounds like these improvements mainly come from circumventing the Python overhead (compiling the code entirely in C?) and reducing GPU \"read/write\". On this second point, basically each time a computation happens in the code, we need to bus data from the GPU memory to the GPU itself for processing and then back to the memory.\\\n",
    "This can become really expensive even when we have very high bandwidth memory because we may end up needing to do a very large amount of transfers. For example, a fairly simple statement like the following could have five separate passes:\\\n",
    "`x = 0.5 * x * (1 - torch.tanh(math.sqrt(2 / math.pi) * x))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8cf69",
   "metadata": {},
   "source": [
    "The compiler batches these operations so that many or all of them are applied at once on the GPU without the need to bus back and forth to the memory for each operation. Apparently this is an example of ***Kernel Fusion***... may be worth reading about"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e5262",
   "metadata": {},
   "source": [
    "It sounds like the gist of kernel fusion is using the small amount of memory (SRAM) that lives in the GPU to do sets of the operations for us instead of streaming between the HBM that is off of the GPU chip for each operation. So, maybe it's not too far off to think of this as chunkating the stream from HBM to GPU chip, processing a set of operations on the GPU chip, then streaming back - rather than streaming one chunk per operation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05349b0",
   "metadata": {},
   "source": [
    "There are some operations that `torch.compile()` **does not find**\n",
    "\n",
    "Enter FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e693d",
   "metadata": {},
   "source": [
    "## FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9d944",
   "metadata": {},
   "source": [
    "From the [2022 paper](https://arxiv.org/pdf/2205.14135), FlashAttention fundamentally works pretty similarly to `torch.compile()` it seems. Basically, it reduces the read/writes between the GPU and HBM. It goes a step further than `torch.compile` in that it actually rewrites teh attention alogirithm. It explicitly separates $K$, $Q$, $V$ matrices, iteratively loading them to SRAM and writing the matmul outputs to HBM, completely avoiding reading/writing the *full $T\\times T$ attention matrix* to HBM. This entire process actually *costs more TFLOPS* than ordinary attention, but it is *dramatically* faster because it manages the memory read/write operations much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e3b64",
   "metadata": {},
   "source": [
    "Interestingly, FlashAttention built upon online softmax first proposed in a [2018 paper from Nvidia](https://arxiv.org/pdf/1805.02867)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7535bf4",
   "metadata": {},
   "source": [
    "The implementation of FlashAttention in pytorch is super simple: `F.scaled_dot_product_attention()`, that's it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d403c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload after adding FlashAttention\n",
    "load_py(\"gpt_homebrew\")  # will need to recall each time script is updated/saved\n",
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig())\n",
    "gpt.to(device)\n",
    "\n",
    "gpt = torch.compile(gpt)  # recompile after reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e99e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.932065, dt: 25,433.62ms, tok/sec: 322.09\n",
      "step 1, loss: 9.635462, dt: 228.31ms, tok/sec: 35,880.57\n",
      "step 2, loss: 9.057121, dt: 235.18ms, tok/sec: 34,832.91\n",
      "step 3, loss: 8.782685, dt: 249.91ms, tok/sec: 32,780.10\n",
      "step 4, loss: 8.509007, dt: 244.93ms, tok/sec: 33,445.99\n",
      "step 5, loss: 8.315421, dt: 233.01ms, tok/sec: 35,156.71\n",
      "step 6, loss: 8.074077, dt: 236.29ms, tok/sec: 34,669.51\n",
      "step 7, loss: 7.929491, dt: 243.47ms, tok/sec: 33,646.60\n",
      "step 8, loss: 7.672122, dt: 244.01ms, tok/sec: 33,571.91\n",
      "step 9, loss: 7.453282, dt: 239.39ms, tok/sec: 34,219.81\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    # add BF16 autocast\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c999b6",
   "metadata": {},
   "source": [
    "So, that's about a 40% increase in speed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121c8e9",
   "metadata": {},
   "source": [
    "## Powers of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5e049",
   "metadata": {},
   "source": [
    "Powers of 2 are nice numbers for NNs. This is because most low-level operations happen in powers of twos (like if we partition matmul operations between tensorcores which do $4\\times 4$ matrix operations). When we don't have powers of twos, then more leg-work is done at a low-level.\n",
    "\n",
    "In our case, a bad number is the ***vocab size*** of 50,257. Very baaadddd.\\\n",
    "The hackneyed solution which actually works is just to find ways to increase numbers to the nearest nice number (i.e. a number that can be factorized into many powers of two). So, we can just override the vocabulary size to be 50,304, a nice number, and get better efficiency. This will just add input dimensions to the embedding and output dimensions to the final output classification head, but that's fine since it will be more efficient. Since no input tokens will use these additional embeddings, we don't really need to care about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46fed2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig(vocab_size=50304))  # nice number\n",
    "gpt.to(device)\n",
    "\n",
    "gpt = torch.compile(gpt)  # recompile after reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655a7be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.969835, dt: 23,063.76ms, tok/sec: 355.19\n",
      "step 1, loss: 9.600793, dt: 222.01ms, tok/sec: 36,899.41\n",
      "step 2, loss: 9.291704, dt: 221.79ms, tok/sec: 36,935.15\n",
      "step 3, loss: 8.853453, dt: 239.39ms, tok/sec: 34,220.43\n",
      "step 4, loss: 8.704416, dt: 239.32ms, tok/sec: 34,230.86\n",
      "step 5, loss: 8.334323, dt: 227.06ms, tok/sec: 36,077.91\n",
      "step 6, loss: 8.144485, dt: 228.03ms, tok/sec: 35,925.67\n",
      "step 7, loss: 7.994779, dt: 229.42ms, tok/sec: 35,706.67\n",
      "step 8, loss: 7.777615, dt: 237.81ms, tok/sec: 34,447.79\n",
      "step 9, loss: 7.580957, dt: 233.12ms, tok/sec: 35,140.74\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    # add BF16 autocast\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931162fb",
   "metadata": {},
   "source": [
    "So that's like a ~4-5% improvement..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e7a55",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cc96c",
   "metadata": {},
   "source": [
    "Here, Andrej tries to copy the training hyperparameters from GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "027527bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add betas used in GPT-3\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679248a",
   "metadata": {},
   "source": [
    "Next, they limit the $l2$-norm of ***all gradients*** for *all parameters* to be no more than $1.0$. Pytorch makes this easy with `torch.nn.utils.clip_grad_norm_`. Basically, the $l2$-norms for each layer of gradients are computed, squared, summed, and rooted - this is equivalent to collecting all of the gradients into a single vector and computing the $l2$-norm of that vector:\n",
    "$$\n",
    "G = \\sqrt{\\sum_{i=1}^k \\| \\nabla L(w_i)\\|_2^2}\n",
    "$$\n",
    "Then, if the total gradient norm exceeds a `max_norm` $C$, the gradients are simply scaled down by:\n",
    "$$ \\alpha = \\frac{C}{G} $$\n",
    "Yielding:\n",
    "$$\\nabla L(w_i)_{\\text{clipped}} = \\alpha \\cdot \\nabla L(w_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109abe47",
   "metadata": {},
   "source": [
    "This basically just helps to reduce the risk of exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 7.428323, dt: 462.63ms, tok/sec: 17,707.33, grad norm: 1.5343868732452393\n",
      "step 1, loss: 8.012007, dt: 229.21ms, tok/sec: 35,739.91, grad norm: 14.025282859802246\n",
      "step 2, loss: 7.422108, dt: 227.86ms, tok/sec: 35,951.23, grad norm: 3.2190914154052734\n",
      "step 3, loss: 7.007826, dt: 229.67ms, tok/sec: 35,669.00, grad norm: 2.499519109725952\n",
      "step 4, loss: 6.743257, dt: 237.70ms, tok/sec: 34,463.96, grad norm: 1.6739602088928223\n",
      "step 5, loss: 6.769724, dt: 243.08ms, tok/sec: 33,701.45, grad norm: 3.9348113536834717\n",
      "step 6, loss: 6.550073, dt: 241.84ms, tok/sec: 33,873.68, grad norm: 1.6428518295288086\n",
      "step 7, loss: 6.547624, dt: 237.44ms, tok/sec: 34,501.40, grad norm: 1.2088583707809448\n",
      "step 8, loss: 6.518197, dt: 237.17ms, tok/sec: 34,540.18, grad norm: 1.3318595886230469\n",
      "step 9, loss: 6.389904, dt: 239.82ms, tok/sec: 34,158.65, grad norm: 1.3799618482589722\n"
     ]
    }
   ],
   "source": [
    "def train_loop(gpt, dl, optimizer, device, steps=10):\n",
    "    for i in range(steps):\n",
    "        t0 = time.time()\n",
    "\n",
    "        xb, yb = next(iter(dl))\n",
    "\n",
    "        # send to device during training / inference instead of during dataset creation\n",
    "        # this is much more efficient for memory usage and dataloader speed\n",
    "        xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "        # add BF16 autocast\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = gpt(xb, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # add gradient clipping\n",
    "        norm = torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "        # causing us to print time EARLY!\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "        tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "        print(\n",
    "            f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\"\n",
    "            + f\", grad norm: {norm:,.4f}\"\n",
    "        )\n",
    "\n",
    "train_loop(gpt, dl, optimizer, device, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ed0b0",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189276ed",
   "metadata": {},
   "source": [
    "In GPT-3 they use a cosine-decay learning-rate scheduler. This gradually decreases the learning rate over the course of training. Learning-rate decay is useful for allowing us to learn more quickly early in training whil reducing the risks of unstable gradients later in training. One of the neat things about the scheduler that the GPT-3 paper uses is that it starts with a very low learning-rate, presumably this promotes stability in the very early phases of training, where getting bad gradients (or learning the wrong things) early on can be very costly. It very rapidly increases the learning rate to the maximum, and then begins decreasing from there down to 10% of the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c8de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step):\n",
    "    max_lr = 3e-4\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = 10\n",
    "    max_steps = 50\n",
    "\n",
    "    # warm up phase -- linearly increase to max_lr\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    # if past max_steps, do nothing\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    # in-between, do annealing\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)  # interpolate\n",
    "    assert 0 <= decay_ratio <= 1, \"Invalid decay_ratio\"\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # cosine decay coefficient\n",
    "    return min_lr + coeff * (max_lr - min_lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b21d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(gpt, dl, optimizer, device, steps=10):\n",
    "    for i in range(steps):\n",
    "        t0 = time.time()\n",
    "\n",
    "        xb, yb = next(iter(dl))\n",
    "\n",
    "        # send to device during training / inference instead of during dataset creation\n",
    "        # this is much more efficient for memory usage and dataloader speed\n",
    "        xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "        # add BF16 autocast\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = gpt(xb, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # add gradient clipping\n",
    "        norm = torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "\n",
    "        # lr cosine annealing\n",
    "        lr = get_lr(i)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "        # causing us to print time EARLY!\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "        tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "        print(\n",
    "            f\"step {i} | loss: {loss.item():,.6f} | dt: {dt:,.2f}ms | tok/sec: {tokens_per_sec:,.2f}\"\n",
    "            + f\" | grad norm: {norm:,.4f} | lr: {lr:e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e4d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload after adding custom weight decay -- weight decay of 0.1 added to all weights (i.e. not to biases/norms)\n",
    "load_py(\"gpt_homebrew\")  # will need to recall each time script is updated/saved\n",
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig())\n",
    "gpt.to(device)\n",
    "\n",
    "gpt = torch.compile(gpt)  # recompile after reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83499078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0 | loss: 10.996070 | dt: 24,698.77ms | tok/sec: 331.68 | grad norm: 25.7995 | lr: 3.000000e-05\n",
      "step 1 | loss: 9.807817 | dt: 220.69ms | tok/sec: 37,119.82 | grad norm: 10.7909 | lr: 6.000000e-05\n",
      "step 2 | loss: 9.378258 | dt: 228.02ms | tok/sec: 35,926.57 | grad norm: 10.1761 | lr: 9.000000e-05\n",
      "step 3 | loss: 9.805191 | dt: 245.63ms | tok/sec: 33,350.45 | grad norm: 11.4488 | lr: 1.200000e-04\n",
      "step 4 | loss: 9.313437 | dt: 241.86ms | tok/sec: 33,870.98 | grad norm: 4.9837 | lr: 1.500000e-04\n",
      "step 5 | loss: 8.875429 | dt: 221.94ms | tok/sec: 36,910.16 | grad norm: 3.3518 | lr: 1.800000e-04\n",
      "step 6 | loss: 8.916994 | dt: 223.19ms | tok/sec: 36,704.97 | grad norm: 4.7965 | lr: 2.100000e-04\n",
      "step 7 | loss: 8.658592 | dt: 234.16ms | tok/sec: 34,984.46 | grad norm: 2.0000 | lr: 2.400000e-04\n",
      "step 8 | loss: 8.479709 | dt: 241.24ms | tok/sec: 33,957.51 | grad norm: 3.3326 | lr: 2.700000e-04\n",
      "step 9 | loss: 8.254002 | dt: 229.51ms | tok/sec: 35,693.02 | grad norm: 3.2192 | lr: 3.000000e-04\n",
      "step 10 | loss: 8.024773 | dt: 232.13ms | tok/sec: 35,290.82 | grad norm: 2.5697 | lr: 3.000000e-04\n",
      "step 11 | loss: 7.711342 | dt: 229.01ms | tok/sec: 35,771.09 | grad norm: 1.9180 | lr: 2.995838e-04\n",
      "step 12 | loss: 7.611670 | dt: 240.07ms | tok/sec: 34,123.57 | grad norm: 1.9671 | lr: 2.983379e-04\n",
      "step 13 | loss: 7.341585 | dt: 235.16ms | tok/sec: 34,836.37 | grad norm: 1.5676 | lr: 2.962699e-04\n",
      "step 14 | loss: 7.165784 | dt: 234.79ms | tok/sec: 34,891.20 | grad norm: 1.4846 | lr: 2.933926e-04\n",
      "step 15 | loss: 6.915674 | dt: 226.54ms | tok/sec: 36,161.86 | grad norm: 1.4626 | lr: 2.897237e-04\n",
      "step 16 | loss: 6.740067 | dt: 239.18ms | tok/sec: 34,250.14 | grad norm: 2.4653 | lr: 2.852859e-04\n",
      "step 17 | loss: 6.677333 | dt: 238.08ms | tok/sec: 34,408.08 | grad norm: 2.1150 | lr: 2.801064e-04\n",
      "step 18 | loss: 6.547552 | dt: 235.77ms | tok/sec: 34,745.17 | grad norm: 1.7287 | lr: 2.742173e-04\n",
      "step 19 | loss: 6.544526 | dt: 232.09ms | tok/sec: 35,296.62 | grad norm: 0.9745 | lr: 2.676548e-04\n",
      "step 20 | loss: 6.276110 | dt: 235.91ms | tok/sec: 34,725.29 | grad norm: 1.1749 | lr: 2.604594e-04\n",
      "step 21 | loss: 6.369612 | dt: 239.51ms | tok/sec: 34,202.58 | grad norm: 0.7407 | lr: 2.526755e-04\n",
      "step 22 | loss: 6.312002 | dt: 237.28ms | tok/sec: 34,524.70 | grad norm: 0.7106 | lr: 2.443510e-04\n",
      "step 23 | loss: 6.182021 | dt: 235.72ms | tok/sec: 34,753.64 | grad norm: 0.7674 | lr: 2.355373e-04\n",
      "step 24 | loss: 6.171267 | dt: 235.98ms | tok/sec: 34,714.49 | grad norm: 0.6424 | lr: 2.262887e-04\n",
      "step 25 | loss: 6.139693 | dt: 232.25ms | tok/sec: 35,272.23 | grad norm: 0.6347 | lr: 2.166623e-04\n",
      "step 26 | loss: 6.129065 | dt: 237.83ms | tok/sec: 34,444.89 | grad norm: 2.7565 | lr: 2.067173e-04\n",
      "step 27 | loss: 6.182504 | dt: 237.43ms | tok/sec: 34,502.37 | grad norm: 0.7781 | lr: 1.965151e-04\n",
      "step 28 | loss: 6.038554 | dt: 238.67ms | tok/sec: 34,323.66 | grad norm: 0.7302 | lr: 1.861187e-04\n",
      "step 29 | loss: 6.260947 | dt: 238.35ms | tok/sec: 34,370.02 | grad norm: 0.9833 | lr: 1.755920e-04\n",
      "step 30 | loss: 5.952951 | dt: 235.60ms | tok/sec: 34,770.66 | grad norm: 0.8208 | lr: 1.650000e-04\n",
      "step 31 | loss: 6.151489 | dt: 232.62ms | tok/sec: 35,216.27 | grad norm: 0.7218 | lr: 1.544080e-04\n",
      "step 32 | loss: 6.076587 | dt: 236.12ms | tok/sec: 34,693.66 | grad norm: 0.6166 | lr: 1.438813e-04\n",
      "step 33 | loss: 6.064536 | dt: 238.46ms | tok/sec: 34,353.86 | grad norm: 0.6929 | lr: 1.334849e-04\n",
      "step 34 | loss: 6.000575 | dt: 238.49ms | tok/sec: 34,349.88 | grad norm: 0.6906 | lr: 1.232827e-04\n",
      "step 35 | loss: 6.001356 | dt: 238.41ms | tok/sec: 34,360.73 | grad norm: 0.8683 | lr: 1.133377e-04\n",
      "step 36 | loss: 6.036118 | dt: 239.93ms | tok/sec: 34,142.93 | grad norm: 0.8414 | lr: 1.037113e-04\n",
      "step 37 | loss: 6.051441 | dt: 239.39ms | tok/sec: 34,220.05 | grad norm: 0.7533 | lr: 9.446269e-05\n",
      "step 38 | loss: 5.947895 | dt: 240.66ms | tok/sec: 34,039.43 | grad norm: 0.8600 | lr: 8.564899e-05\n",
      "step 39 | loss: 5.976474 | dt: 239.00ms | tok/sec: 34,276.58 | grad norm: 0.8402 | lr: 7.732451e-05\n",
      "step 40 | loss: 5.945938 | dt: 236.18ms | tok/sec: 34,685.26 | grad norm: 0.6593 | lr: 6.954058e-05\n",
      "step 41 | loss: 6.059251 | dt: 240.59ms | tok/sec: 34,050.29 | grad norm: 0.6696 | lr: 6.234519e-05\n",
      "step 42 | loss: 6.001596 | dt: 239.70ms | tok/sec: 34,175.97 | grad norm: 0.6484 | lr: 5.578271e-05\n",
      "step 43 | loss: 5.993316 | dt: 239.74ms | tok/sec: 34,169.69 | grad norm: 0.5486 | lr: 4.989358e-05\n",
      "step 44 | loss: 5.899045 | dt: 237.15ms | tok/sec: 34,544.24 | grad norm: 0.6445 | lr: 4.471412e-05\n",
      "step 45 | loss: 5.844029 | dt: 239.02ms | tok/sec: 34,273.27 | grad norm: 0.7859 | lr: 4.027626e-05\n",
      "step 46 | loss: 5.922193 | dt: 235.58ms | tok/sec: 34,773.19 | grad norm: 0.8637 | lr: 3.660737e-05\n",
      "step 47 | loss: 5.936824 | dt: 235.41ms | tok/sec: 34,798.41 | grad norm: 0.6444 | lr: 3.373006e-05\n",
      "step 48 | loss: 5.926616 | dt: 237.92ms | tok/sec: 34,431.87 | grad norm: 0.7330 | lr: 3.166207e-05\n",
      "step 49 | loss: 5.913262 | dt: 241.20ms | tok/sec: 33,962.81 | grad norm: 0.5704 | lr: 3.041616e-05\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "\n",
    "# weight decay from paper\n",
    "optimizer = gpt.configure_optimizers(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95), device_type='cuda')\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4, betas=(0.9, 0.95))\n",
    "\n",
    "train_loop(gpt, dl, optimizer, device, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64635b51",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cbb11",
   "metadata": {},
   "source": [
    "GPT-3 Small was trained with batches of ~0.5M tokens, or ~488 sequences each of length 1,024.\\\n",
    "We obviously cannot directly replicate this batch size because it won't fit in our available memory. But we can simulate this batch size using ***Gradient Accumulation***\n",
    "\n",
    "This sounds fancy, but it actually just amounts to getting gradients for forward passes through multiple batches, tracking them all, then updating at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725e25f",
   "metadata": {},
   "source": [
    "This is pretty simple to implement because the `.backwards()` method already does gradient accumulation - i.e. it just ADDS gradients (this is why we always call optimizer.zero_grad() ofc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce94a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with total_batch_size < 0.5M for time and since tinyshakespeare may be too small for that...\n",
    "def accum_train_loop(gpt, dl, optimizer, device, steps: int = 10, total_batch_size: int = 65536):\n",
    "    \"\"\"Training loop with gradient accumulation\"\"\"\n",
    "    assert total_batch_size % (dl.batch_size * gpt.config.block_size) == 0, \"total_batch_size not evenly divisible by minibatches\"\n",
    "    accum_steps = total_batch_size // (dl.batch_size * gpt.config.block_size)\n",
    "\n",
    "    for i in range(steps):\n",
    "        t0 = time.time()\n",
    "        loss_accum = 0.0\n",
    "        for acc_step in range(accum_steps):\n",
    "            xb, yb = next(iter(dl))\n",
    "\n",
    "            # send to device during training / inference instead of during dataset creation\n",
    "            # this is much more efficient for memory usage and dataloader speed\n",
    "            xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "            # add BF16 autocast\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = gpt(xb, yb)\n",
    "            \n",
    "            loss /= accum_steps  # <-- normalize to account for smaller minibatch size (since cross_entropy has mean reduction by default)\n",
    "            loss_accum += loss.detach()\n",
    "            loss.backward()  # this AUTOMATICALLY accumulates gradients in the gradient tensors - just adds them\n",
    "\n",
    "        # add gradient clipping\n",
    "        norm = torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "\n",
    "        # lr cosine annealing\n",
    "        lr = get_lr(i)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "        # causing us to print time EARLY!\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "        tokens_per_sec = (dl.batch_size * xb.shape[1] * accum_steps) / (t1 - t0)\n",
    "        print(\n",
    "            f\"step {i} | loss: {loss_accum.item():,.6f} | dt: {dt:,.2f}ms | tok/sec: {tokens_per_sec:,.2f}\"\n",
    "            + f\" | grad norm: {norm:,.4f} | lr: {lr:e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb80e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0 | loss: 10.896723 | dt: 15,714.33ms | tok/sec: 33,363.68 | grad norm: 0.4496 | lr: 3.000000e-05\n",
      "step 1 | loss: 9.660885 | dt: 14,404.30ms | tok/sec: 36,398.01 | grad norm: 0.1724 | lr: 6.000000e-05\n",
      "step 2 | loss: 9.324739 | dt: 14,082.58ms | tok/sec: 37,229.53 | grad norm: 0.2252 | lr: 9.000000e-05\n",
      "step 3 | loss: 9.688659 | dt: 13,727.82ms | tok/sec: 38,191.65 | grad norm: 0.1484 | lr: 1.200000e-04\n",
      "step 4 | loss: 9.312862 | dt: 13,746.38ms | tok/sec: 38,140.08 | grad norm: 0.0870 | lr: 1.500000e-04\n",
      "step 5 | loss: 9.016571 | dt: 13,710.54ms | tok/sec: 38,239.78 | grad norm: 0.0662 | lr: 1.800000e-04\n",
      "step 6 | loss: 8.772388 | dt: 13,764.61ms | tok/sec: 38,089.56 | grad norm: 0.0467 | lr: 2.100000e-04\n",
      "step 7 | loss: 8.618524 | dt: 13,873.31ms | tok/sec: 37,791.13 | grad norm: 0.0336 | lr: 2.400000e-04\n",
      "step 8 | loss: 8.483671 | dt: 13,930.70ms | tok/sec: 37,635.43 | grad norm: 0.0506 | lr: 2.700000e-04\n",
      "step 9 | loss: 8.306776 | dt: 13,890.03ms | tok/sec: 37,745.62 | grad norm: 0.0496 | lr: 3.000000e-04\n"
     ]
    }
   ],
   "source": [
    "# reload after adding custom weight decay -- weight decay of 0.1 added to all weights (i.e. not to biases/norms)\n",
    "load_py(\"gpt_homebrew\")  # will need to recall each time script is updated/saved\n",
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig())\n",
    "gpt.to(device)\n",
    "\n",
    "gpt = torch.compile(gpt)  # recompile after reloading\n",
    "\n",
    "dl = DataLoader(train, batch_size=8, shuffle=True)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "\n",
    "# weight decay from paper\n",
    "optimizer = gpt.configure_optimizers(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95), device_type='cuda')\n",
    "\n",
    "accum_train_loop(gpt, dl, optimizer, device, steps=10, total_batch_size=524288) # // 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb64dd",
   "metadata": {},
   "source": [
    "## Parallelizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a26269",
   "metadata": {},
   "source": [
    "Can parallelize across GPUs in pytorch by using `DistributedDataParallel`.\\\n",
    "Each GPU will do forward and backwards passes in parallel. But the gradients that they use to update parameters will be the average of the gradients computed in the parallel backwards passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc38bcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# copy pasta\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "# set up DDP (distributed data parallel).\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])              # int ID of each process across all nodes\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])  # local rank (int ID) of process on this node - same as rank then\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])  # number of processes across all nodes - usually = number of GPUs\n",
    "    device = f'cuda:{ddp_local_rank}'  # set device to cuda:<local_rank> -- get correct GPU\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run -- this is single-GPU scenario\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    # attempt to autodetect device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad35310",
   "metadata": {},
   "source": [
    "Need to adjust accumulation for parallelization *and* dataloader.\\\n",
    "Using pytorch `DataLoader`, we can adjust for DDP by adding a `DistributedSampler` sampler. This partitions the dataset across the GPUs (ranks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0337ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DistributedSampler\n",
    "sampler = DistributedSampler(train, num_replicas=ddp_world_size, rank=ddp_rank, shuffle=True)\n",
    "dl = DataLoader(train, batch_size=8, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "884addda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model with GPT\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)  # set common seed for all GPUs so they all init the SAME model\n",
    "gpt = gpt_homebrew.GPT(gpt_homebrew.GPTConfig()).to(device)\n",
    "gpt.compile()\n",
    "\n",
    "# wrap model in DDP\n",
    "if ddp:\n",
    "    gpt = DDP(gpt, device_ids=[ddp_local_rank])\n",
    "\n",
    "raw_gpt = gpt.module if ddp else gpt  # unwrap DDP for optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9e516",
   "metadata": {},
   "source": [
    "DDP will synchronize gradients for us by averaging them across the GPUs before updating. So, we have separate but identical models living in each GPU, all processing separate batches of the training data but receiving *the same parameter updates*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1de884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accum_steps: 64 (total_batch_size: 524288, dl.batch_size: 8, block_size: 1024, ddp_world_size: 1)\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0 | loss: 11.011212 | dt: 14,185.72ms | tok/sec: 36,958.85 | grad norm: 0.4259 | lr: 3.000000e-05\n",
      "step 1 | loss: 9.682494 | dt: 14,096.88ms | tok/sec: 37,191.78 | grad norm: 0.1992 | lr: 6.000000e-05\n",
      "step 2 | loss: 9.236091 | dt: 13,718.60ms | tok/sec: 38,217.31 | grad norm: 0.1986 | lr: 9.000000e-05\n",
      "step 3 | loss: 9.449409 | dt: 13,486.06ms | tok/sec: 38,876.28 | grad norm: 0.1612 | lr: 1.200000e-04\n",
      "step 4 | loss: 9.027285 | dt: 13,539.00ms | tok/sec: 38,724.28 | grad norm: 0.0805 | lr: 1.500000e-04\n",
      "step 5 | loss: 8.736452 | dt: 13,558.33ms | tok/sec: 38,669.07 | grad norm: 0.0601 | lr: 1.800000e-04\n",
      "step 6 | loss: 8.529930 | dt: 13,681.62ms | tok/sec: 38,320.61 | grad norm: 0.0401 | lr: 2.100000e-04\n",
      "step 7 | loss: 8.378812 | dt: 13,779.48ms | tok/sec: 38,048.45 | grad norm: 0.0434 | lr: 2.400000e-04\n",
      "step 8 | loss: 8.182464 | dt: 13,810.21ms | tok/sec: 37,963.81 | grad norm: 0.0458 | lr: 2.700000e-04\n",
      "step 9 | loss: 7.928040 | dt: 13,765.96ms | tok/sec: 38,085.82 | grad norm: 0.0301 | lr: 3.000000e-04\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288\n",
    "steps = 10\n",
    "\n",
    "# adjust accumulation steps for DDP\n",
    "assert total_batch_size % (dl.batch_size * gpt.config.block_size * ddp_world_size) == 0, \\\n",
    "    \"total_batch_size not evenly divisible by minibatches\"\n",
    "accum_steps = total_batch_size // (dl.batch_size * gpt.config.block_size * ddp_world_size)  # distribute\n",
    "if master_process:\n",
    "    print(f\"accum_steps: {accum_steps} (total_batch_size: {total_batch_size}, \"\n",
    "          f\"dl.batch_size: {dl.batch_size}, block_size: {gpt.config.block_size}, \"\n",
    "          f\"ddp_world_size: {ddp_world_size})\")\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "optimizer = raw_gpt.configure_optimizers(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95), device_type='cuda')\n",
    "for i in range(steps):\n",
    "    t0 = time.time()\n",
    "    loss_accum = 0.0\n",
    "    for acc_step in range(accum_steps):\n",
    "        xb, yb = next(iter(dl))\n",
    "\n",
    "        # send to device during training / inference instead of during dataset creation\n",
    "        # this is much more efficient for memory usage and dataloader speed\n",
    "        xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "        # add BF16 autocast\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = gpt(xb, yb)\n",
    "        \n",
    "        loss /= accum_steps  # <-- normalize to account for smaller minibatch size (since cross_entropy has mean reduction by default)\n",
    "        loss_accum += loss.detach()\n",
    "\n",
    "        # under ddp, only sync gradients on LAST accumulation step\n",
    "        # without this, ddp would automatically sync after EVERY backward()\n",
    "        if ddp:\n",
    "            gpt.requires_backward_grad_sync = (acc_step == accum_steps - 1)\n",
    "\n",
    "        loss.backward()  # this AUTOMATICALLY accumulates gradients in the gradient tensors - just adds them\n",
    "\n",
    "    # this averages loss_accum across all DDP processes\n",
    "    if ddp:\n",
    "        dist.allr_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "\n",
    "    # add gradient clipping\n",
    "    norm = torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "\n",
    "    # lr cosine annealing\n",
    "    lr = get_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1] * accum_steps * ddp_world_size) / (t1 - t0)\n",
    "\n",
    "    # only master process does logging\n",
    "    if master_process:\n",
    "        print(\n",
    "            f\"step {i} | loss: {loss_accum.item():,.6f} | dt: {dt:,.2f}ms | tok/sec: {tokens_per_sec:,.2f}\"\n",
    "            + f\" | grad norm: {norm:,.4f} | lr: {lr:e}\"\n",
    "        )\n",
    "\n",
    "# clean up after training\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b95d61",
   "metadata": {},
   "source": [
    "Were we to actually run this in a distributed way, we could use:\\\n",
    "`torchrun --standalone --nproc_per_node=8 train_gpt2.py` or similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25523ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
