{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afe4a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in ./.local/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.local/lib/python3.10/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.local/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dd6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6825649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device='mps'\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fad3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data manager for tinyshakespeare data\n",
    "class ShakeMgr:\n",
    "    def __init__(self, seq_len: int = 32):\n",
    "        if 'tinyshakespeare_data' not in globals():\n",
    "            import urllib.request\n",
    "            url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "            # read text in from url\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                tinyshakespeare_data = response.read().decode('utf-8')\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.text = tinyshakespeare_data\n",
    "        self.tokens = enc.encode(self.text)\n",
    "\n",
    "        # stack token sequences into data buffer for dataloader\n",
    "        buf_len  = len(self.tokens) - (len(self.tokens) % (seq_len + 1))\n",
    "        self.buffer = self.tokens[:buf_len]\n",
    "        self.buffer = torch.tensor(self.buffer).reshape(-1, seq_len + 1)\n",
    "\n",
    "    def train_test_split(self, train_pct: float = 0.8):\n",
    "        idx = np.random.permutation(data_len := len(self.buffer))\n",
    "        train_data = self.buffer[idx[:int(data_len * train_pct)]]\n",
    "        test_data = self.buffer[idx[int(data_len * train_pct):]]\n",
    "\n",
    "        return ShakeDS(train_data), ShakeDS(test_data)  # return train-test split\n",
    "\n",
    "# dataset class for tinyshakespeare data, compatible with pytorch dataloader\n",
    "class ShakeDS(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.data[idx, :-1]  # shifted input sequence\n",
    "        y = self.data[idx, 1:]   # shifted target sequence\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1e7e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_homebrew'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-127727385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgpt_homebrew\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# gpt = gh.GPT.from_pretrained('gpt2', lora_rank=0, lora_alpha=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_homebrew'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gpt_homebrew as gh\n",
    "from importlib import reload\n",
    "reload(gh)\n",
    "\n",
    "# gpt = gh.GPT.from_pretrained('gpt2', lora_rank=0, lora_alpha=16)\n",
    "gpt = gh.GPT(gh.GPTConfig())\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a094e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new training data with maximum batch-size sequences\n",
    "data = ShakeMgr(seq_len=1024)  # max batch-size\n",
    "train, test = data.train_test_split(train_pct = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c0507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 6.181458950042725, dt: 294.39ms, tok/sec: 41,741.06\n",
      "step 1, loss: 6.43180513381958, dt: 287.68ms, tok/sec: 42,714.21\n",
      "step 2, loss: 6.429523468017578, dt: 287.77ms, tok/sec: 42,700.16\n",
      "step 3, loss: 6.273110866546631, dt: 287.57ms, tok/sec: 42,729.79\n",
      "step 4, loss: 6.255056858062744, dt: 287.65ms, tok/sec: 42,719.03\n",
      "step 5, loss: 6.231906414031982, dt: 287.64ms, tok/sec: 42,720.41\n",
      "step 6, loss: 6.144219875335693, dt: 287.59ms, tok/sec: 42,727.21\n",
      "step 7, loss: 6.330781936645508, dt: 287.70ms, tok/sec: 42,711.52\n",
      "step 8, loss: 6.293788433074951, dt: 287.59ms, tok/sec: 42,727.17\n",
      "step 9, loss: 6.168107986450195, dt: 287.69ms, tok/sec: 42,713.29\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train, batch_size=12, shuffle=True)\n",
    "\n",
    "# try adjusting precision of pytorch float32 operations\n",
    "# setting to 'high' uses TensorFlow32 (TF32) which subtley adjusts dtypes in operations\n",
    "# increases TFLOPS on GPUs with tensorcores (like A100) by ~8X -- may improve other GPUs too\n",
    "# TF32 is available on GPUs after the \"Ampere\" series (of which A100 is one)\n",
    "torch.set_float32_matmul_precision('high')  # 'highest' is default\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "import time\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    logits, loss = gpt(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789c8d5",
   "metadata": {},
   "source": [
    "Adjusting to use TF32 in matmul operations increased approximate tokens per second from ~15.7k --> ~42.7k.\\\n",
    "This is less than the ~8k improvement suggested by TFLOPS because of memory constraints.\n",
    "\n",
    "We can do better by improving memory efficiency - i.e. using a floating-point format for the parameter data itself, not just the operation (a bit arcane......). The trade off is reduced precision and potentially a reduced range of value expressions. The format BF16 is a good middle-ground. It maintains the range of expression of FP32 (the default for `float32` dtype), but reduces the precision to reduce the bit-cost in memory. \n",
    "\n",
    "[This tutorial](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html) explains the pytorch implementation for adjusting float formats, particularly using `torch.autocast`. This can be used to adust the formats during the forward pass automatically.\\\n",
    "This only converts some datatypes. The implementation used here won't convert the embedding weights. It sounds like the `nn.Linear` layers will be changed which is good since they tend to be the most expensive here.\n",
    "\n",
    "**Again**: this is only possible to do in Ampere GPUs, older GPUs don't support BF16 (I guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdae54d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 6.168858, dt: 414.19ms, tok/sec: 29,667.73\n",
      "step 1, loss: 6.237749, dt: 257.91ms, tok/sec: 47,645.27\n",
      "step 2, loss: 6.250380, dt: 257.77ms, tok/sec: 47,671.32\n",
      "step 3, loss: 6.187717, dt: 257.81ms, tok/sec: 47,663.38\n",
      "step 4, loss: 6.121225, dt: 257.66ms, tok/sec: 47,691.17\n",
      "step 5, loss: 6.128353, dt: 258.01ms, tok/sec: 47,626.83\n",
      "step 6, loss: 6.067403, dt: 257.78ms, tok/sec: 47,669.29\n",
      "step 7, loss: 6.074363, dt: 258.20ms, tok/sec: 47,591.73\n",
      "step 8, loss: 6.043103, dt: 257.84ms, tok/sec: 47,657.96\n",
      "step 9, loss: 6.045007, dt: 257.63ms, tok/sec: 47,696.60\n"
     ]
    }
   ],
   "source": [
    "# add BF16 autocast\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xb, yb = next(iter(dl))\n",
    "\n",
    "    # send to device during training / inference instead of during dataset creation\n",
    "    # this is much more efficient for memory usage and dataloader speed\n",
    "    xb, yb = xb.to(device), yb.to(device)  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = gpt(xb, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # this waits for the GPU to finish operations - otherwise CPU REPL stuff may finish before GPU\n",
    "    # causing us to print time EARLY!\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in miliseconds\n",
    "    tokens_per_sec = (dl.batch_size * xb.shape[1]) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item():,.6f}, dt: {dt:,.2f}ms, tok/sec: {tokens_per_sec:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafa678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
