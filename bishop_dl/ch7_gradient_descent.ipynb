{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch; import torch.nn as nn; import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Error Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a change in the weights from $\\bf w$ to $\\mathbf{w} + \\delta \\mathbf{w}$. Such a change, will correspond to a change in the error function $E(\\bf w)$ that is proportional to its gradient with respect to $\\bf w$. That is:\n",
    "$$\\delta E \\simeq \\delta \\mathbf{w}^\\intercal \\nabla E(\\bf w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For smooth and convex $E(\\bf w)$, minima will occur where: $$ \\nabla E(\\mathbf{w}) = \\mathbf{0}$$\n",
    "In principal then, we aim to find minima by iteratively scaling the parameters (e.g. weights) in the direction of $-\\nabla E(\\bf w)$\\\n",
    " Well, really we may reach a minima, maxima, or saddle point at points where the gradient vanishes. And indeed, we are typically concerned with high dimensional spaces and error functions with highly nonlinear dependencies on network parameters, so it will often be the case that many minima, maxima, and saddle points exist. Moreover, for any given minima we may generally find many equivalent minima within the parameter space.\n",
    "\n",
    "While we may rarely be able to hope to find the global minimum, we can get very good results by simply finding sufficient minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Quadratic Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section motivates gradient descent by discussing an approximation to the Newton-Raphson optimization algorithm which I've written about [here](https://github.com/BenAF002/data_science/blob/main/Notes/maths_notes/Newton_optimizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a point in the weight space $\\hat{\\bf w}$. The Second-Order Taylor Series expansion (recall that Newton-Raphson only uses expansions of second-order) of $E(\\bf w)$ around this point is:\n",
    "$$E(\\mathbf{w}) \\simeq E(\\hat{\\mathbf{w}}) + \\bf (w - \\hat{w})^\\intercal b + \\frac{1}{2}(w - \\hat{w})^\\intercal H(w - \\hat{\\mathbf{w}})$$\n",
    "Where $\\bf b$ is defined as the gradient of $E$ w.r.t. $\\bf w$ evaluated at $\\hat{\\bf w}$ and $\\bf H$ is the Hessian matrix:\n",
    "$$\\mathbf{b} \\equiv \\nabla E|_{\\mathbf{w} = \\hat{\\mathbf{w}}} \\\\  \\\\ \\mathbf{H}(\\hat{\\mathbf{w}}) = \\nabla \\nabla E(\\bf w)|_{w=\\hat{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local approximation of the gradient from the Taylor Series expansion is:\n",
    "$$\\nabla E(\\bf w) = b + H(w - \\hat{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the discussion is a little convoluted and, I feel, unnecessary. What's important to glean from it is\n",
    "> A necessary and sufficient condition for $\\bf w^*$ to be a local minimum is that $\\nabla E(\\bf w) = 0$ *and* the Hessian $\\bf H$ is positive definite (i.e. $\\bf x^\\intercal Hx = 0, \\ \\forall x$ or equivalently, all of the eigenvalues of $\\bf H$ are positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**:\\\n",
    "The fact that we may determine positive definiteness from the eigenvalues of the Hessian is deducible as follows:\n",
    "- The Hessian $\\bf H$ is a square, symmetric matrix; thus it always has real eigenvalues and a complete set of eigenvectors $\\{\\mathbf{u}_i\\}$\n",
    "- Because the eigenvectors of the Hessian form a complete set, they may represent any arbitrary vector $\\bf v$ in the vector space spanned by the Hessian as:\n",
    "$$\\mathbf{v} = \\sum_i c_i \\mathbf{u}_i$$\n",
    "- $\\bf H$ is positive definite if and only if $\\bf v^\\intercal H v > 0, \\ \\ \\forall v$\n",
    "    - Equivalently, if and only if $\\mathbf{v}^\\intercal \\mathbf{H} \\mathbf{v} = \\sum_i c_i^2 \\lambda_i > 0, \\ \\ \\forall \\lambda_i$\n",
    "So, if all of the eigenvalues of $\\bf H$, $\\lambda_i$ are positive, then the Hessian is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic approach:\n",
    "$$\\mathbf{w}^{(\\tau)} = \\mathbf{w}^{(\\tau - 1)} + \\Delta\\mathbf{w}^{(\\tau - 1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a great deal of nuance involved in both the selection of the weight vector update $\\Delta\\mathbf{w}^{(\\tau)}$ and the weight initializations $\\mathbf{w}^{(0)}$, as both of these things can have a very large impact on the solution found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach to updating with gradient information is to choose the weight update such that there is a small step in the direction of the negative gradient (of the error function w.r.t. the parameters).\n",
    "$$\\mathbf{w}^{(\\tau)} = \\mathbf{w}^{(\\tau - 1)} - \\eta \\nabla E(\\mathbf{w}^{(\\tau - 1)})$$\n",
    "Where $\\eta > 0$ is tunable hyperparameter called the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we define the error function over the entire training set (e.g., often we sum over $N$). So, this approach to iterative parameter updating requires evaluating with the entire training dataset. Techniques that use the whole data set at once are called ***Batch Methods***.\\\n",
    "(This is a little annoying bc I usually think of \"batches\" as minibatches and not the full training dataset)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the entire training dataset at once can be very inefficient when the training dataset is large. So, we can improve efficiency by splitting up the dataset into *minibatches* and training over those instead. At the most granular level, we could have $N$ minibatches, each of size 1, such that each individual observation is treated as a minibatch. Then,\n",
    "$$E(\\mathbf{w}) = \\sum_{n=1}^NE_n(\\mathbf{w}) \\\\ \\\\ \\mathbf{w}^{(\\tau)} = \\mathbf{w}^{(\\tau -1)} - \\eta \\nabla E_n(\\mathbf{w}^{(\\tau - 1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the crux of SGD. An ***Epoch*** is then a complete pass through the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other benefits to SGD besides reducing computational complexity at each iteration. One is that it reduces the risk of getting stuck on a poor local minimum or saddle point because stationary points w.r.t. the entire training dataset will generally not be stationary points w.r.t. smaller subsets of the training data. Another is that it can help improve the speed of parameter optimization. Consider the gradient of the error function w.r.t. the entire training dataset. The partial derivatives of each parameter (e.g. each weight in $\\bf w$) represents a sort of average relationship between that parameter and the error function at every input value. So, different relationships that may be observed at different points in the input space become muted or offset one-another in the gradient. Thus, the update at each step does not change the parameter value by as much as it otherwise could when using smaller minibatches (I think)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second benefit that I noted can prove to be a downside when we train with very few observations at each iteration. The gradient of the error function computed from a single data point is a very noisy estimate of the gradient computed on the full data set, and too much noise can cause parameters to vary too much. An intermediate approach is to use *mini-batches* of size greater than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution obtained through gradient descent is heavily dependent upon parameter initialization.\n",
    "\n",
    "One key consideration is the concept of *symmetry breaking*. We don't want parameters that are constant (e.g. all initialized to 0), because then they will all comput the same output values and be completely redundant. Similarly, we don't want systemic trends in the parameter initializations for the same reason - i.e. to avoid redundant parameters that *arbitrarily* produce similar outputs and therefore *arbitrarily* move together when updated through gradient descent.\n",
    "\n",
    "So, we want to initialize parameters randomly.\n",
    "\n",
    "Additionally, if we are using ReLU activations, then we should be careful to ensure that most initial pre-activations (at least in the early layers of the network) are positive so that we don't prematurely kill neurons. One way to do this without systematically biasing the weight initializations is to initialize the bias parameters as small positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
