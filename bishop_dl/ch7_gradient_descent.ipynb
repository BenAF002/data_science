{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch; import torch.nn as nn; import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Error Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a change in the weights from $\\bf w$ to $\\mathbf{w} + \\delta \\mathbf{w}$. Such a change, will correspond to a change in the error function $E(\\bf w)$ that is proportional to its gradient with respect to $\\bf w$. That is:\n",
    "$$\\delta E \\simeq \\delta \\mathbf{w}^\\intercal \\nabla E(\\bf w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For smooth and convex $E(\\bf w)$, minima will occur where: $$ \\nabla E(\\mathbf{w}) = \\mathbf{0}$$\n",
    "In principal then, we aim to find minima by iteratively scaling the parameters (e.g. weights) in the direction of $-\\nabla E(\\bf w)$\\\n",
    " Well, really we may reach a minima, maxima, or saddle point at points where the gradient vanishes. And indeed, we are typically concerned with high dimensional spaces and error functions with highly nonlinear dependencies on network parameters, so it will often be the case that many minima, maxima, and saddle points exist. Moreover, for any given minima we may generally find many equivalent minima within the parameter space.\n",
    "\n",
    "While we may rarely be able to hope to find the global minimum, we can get very good results by simply finding sufficient minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Quadratic Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section motivates gradient descent by discussing an approximation to the Newton-Raphson optimization algorithm which I've written about [here](https://github.com/BenAF002/data_science/blob/main/Notes/maths_notes/Newton_optimizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a point in the weight space $\\hat{\\bf w}$. The Second-Order Taylor Series expansion (recall that Newton-Raphson only uses expansions of second-order) of $E(\\bf w)$ around this point is:\n",
    "$$E(\\mathbf{w}) \\simeq E(\\hat{\\mathbf{w}}) + \\bf (w - \\hat{w})^\\intercal b + \\frac{1}{2}(w - \\hat{w})^\\intercal H(w - \\hat{\\mathbf{w}})$$\n",
    "Where $\\bf b$ is defined as the gradient of $E$ w.r.t. $\\bf w$ evaluated at $\\hat{\\bf w}$ and $\\bf H$ is the Hessian matrix:\n",
    "$$\\mathbf{b} \\equiv \\nabla E|_{\\mathbf{w} = \\hat{\\mathbf{w}}} \\\\  \\\\ \\mathbf{H}(\\hat{\\mathbf{w}}) = \\nabla \\nabla E(\\bf w)|_{w=\\hat{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local approximation of the gradient from the Taylor Series expansion is:\n",
    "$$\\nabla E(\\bf x) = b + H(w - \\hat{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the discussion is a little convoluted and, I feel, unnecessary. What's important to glean from it is\n",
    "> A necessary and sufficient condition for $\\bf w^*$ to be a local minimum is that $\\nabla E(\\bf w) = 0$ *and* the Hessian $\\bf H$ is positive definite (i.e. $\\bf x^\\intercal Hx = 0, \\ \\forall x$ or equivalently, all of the eigenvalues of $\\bf H$ are positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**:\\\n",
    "The fact that we may determine positive definiteness from the eigenvalues of the Hessian is deducible as follows:\n",
    "- The Hessian $\\bf H$ is a square, symmetric matrix; thus it always has real eigenvalues and a complete set of eigenvectors $\\{\\mathbf{u}_i\\}$\n",
    "- Because the eigenvectors of the Hessian form a complete set, they may represent any arbitrary vector $\\bf v$ in the vector space spanned by the Hessian as:\n",
    "$$\\mathbf{v} = \\sum_i c_i \\mathbf{u}_i$$\n",
    "- $\\bf H$ is positive definite if and only if $\\bf v^\\intercal H v > 0, \\ \\ \\forall v$\n",
    "    - Equivalently, if and only if $\\mathbf{v}^\\intercal \\mathbf{H} \\mathbf{v} = \\sum_i c_i^2 \\lambda_i > 0, \\ \\ \\forall \\lambda_i$\n",
    "So, if all of the eigenvalues of $\\bf H$, $\\lambda_i$ are positive, then the Hessian is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
