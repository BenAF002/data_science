{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Rules of Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Uncertainty:**\n",
    "- *Epistemic* or *Systematic* Uncertainty\n",
    "    - Uncertainty resulting from imperfect knowledge about data (e.g. limited sample size)\n",
    "    - One way to reduce this uncertainty is by increasing sample size\n",
    "- *Aleatoric* or *Intrinsic* Uncertainty, or *Noise*\n",
    "    - Uncertainty that arises from imperfect knowledge about the world (e.g. perfect prediction requires infinitely precise measurement)\n",
    "    - One way to reduce this uncertainty is by measuring more things (increasing feature space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sum and Product Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given two random variables: $$X = x_i \\ ; \\ i \\in 1,...,L \\\\ Y = y_j \\ ; \\ j \\in 1,...,M$$\n",
    "The number of observations where $X=x_i$ ***and*** $Y=y_j$ is denoted $n_{ij}$. The number of observations where $X=x_i$ irrespective of $Y$ is denoted $c_i$ and the number of observations where $Y=y_j$ irrespective of $X$ is denoted $r_j$. Finally, the total number of observations is $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the joint probability of any combination of $x_i$ and $y_j$ is given by: $$p(X=x_i, Y=y_j) = \\frac{n_{ij}}{N}$$\n",
    "And the marginal probabilities are given by: $$p(X=x_i) = \\frac{c_i}{N} \\\\ \\ \\\\ p(Y=y_j) = \\frac{r_j}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all marginal probabilities over the supports of $X$ or $Y$ must sum to 1: $$\\sum_{i=1}^L p(X=x_i) = 1$$\n",
    "Thus, $$c_i = \\sum_j n_{ij}$$\n",
    "Which yields the ***Sum Rule of Probability***: $$p(X=x_i) = \\sum_{j=1}^M p(X=x_i, Y=y_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional probability is: $$p(Y=y_j|X=x_i) = \\frac{n_{ij}}{c_i}$$\n",
    "The sum over all conditional probabilities must also equal 1: $$\\sum_{j=1}^M p(Y=y_j|X=x_i) = 1$$\n",
    "Now we may derive the ***Product Rule of Probability***:\n",
    "$$p(X=x_i, Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X=x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing these in shorter notation:\n",
    "- **Sum Rule:**  $$p(X) = \\sum_Y p(X,Y)$$\n",
    "- **Product Rule:** $$p(X,Y) = p(Y|X)p(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baye's Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that joint probabilities posess symmetry: $p(X,Y) = p(Y,X)$. Therefore, by the product rule we may express: $$p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y) = p(Y,X)$$\n",
    "So, we may express a conditional probability as: $$p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$$\n",
    "This is ***Baye's Theorem***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Screening Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the RV $C$ denotes the presence of cancer and the RV $T$ represents the outcome of a screening test. We are given: $$p(C=1) = 1/100 \\\\ p(C=0) = 99/100 \\\\ \\ \\\\ p(T=1|C=1) = 90/100 \\\\ p(T=0|C=1) = 10/100 \\\\ p(T=1|C=0) = 3/100 \\\\ p(T=0|C=0) = 97/100$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test is positive, what is the probability that the patient actually has cancer?\n",
    "$$p(C=1|T=1) = \\frac{p(T=1|C=1)p(C=1)}{p(T=1)} = \\frac{p(T=1|C=1)p(C=1)}{p(T=1|C=1)p(C=1) + p(T=1|C=0)p(C=0)} \\\\ \\ \\\\ =\\frac{0.9(0.01)}{0.9(0.01) + 0.03(0.99)} = 0.233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior and Posterior Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cancer screening example, the ***prior probability*** that somebody does have cancer is $p(C)$. It is the probability before we've gained any information by observing the result of a test. Once we have the result of a test, then the ***posterior probability*** is $p(C|T)$. In this example, the prior probability is $1\\%$ while the posterior probability is $23\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two random variables are concerned to be independent if $p(X,Y) = p(X)p(Y)$.\\\n",
    " It follows that, for independent RVs $X$ and $Y$: $$p(Y|X) = p(Y) \\\\ p(X|Y) = p(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Probability Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ***Continuous Random Variable*** $x$, the *probability density function* is given by:\n",
    "$$p(x \\in (a,b)) = \\int_a^b p(x)\\text{d}x$$\n",
    "Satisfying the conditions:\n",
    "1. $p(x) \\ge 0$\n",
    "2. $\\int_{-\\infty}^\\infty p(x)\\text{d}x = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous RVs, we cannot compute the probability that the RV wille be equal to any precise value. So, the probability density $p(x)$ must always be understood as meaning: $$p(x) = p(x \\in (x, x + \\epsilon))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Cumulative Distribution Function*** is given by:\n",
    "$$P(z) = \\int_{-\\infty}^z p(x) \\text{d}x$$\n",
    "Note that eh CDF taken over $\\mathbb{R}$ must equal 1 by condition 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint PDFs:**\n",
    "\n",
    "With several continuous RVs $x_1,..., x_D$ denoted $\\mathbf{x}$, the joint probability density is denoted $p(\\mathbf{x}) = p(x_1,...,x_D)$. Its interpretation is that the probability of $\\mathbf{x}$ falling in an *infinitesimal volume* $\\delta\\mathbf{x}$ containing the point $\\mathbf{x}$ is given by $p(\\mathbf{x})\\delta\\mathbf{x}$.\n",
    "\n",
    "The joint pdf must satisfy these conditions:\n",
    "1. $p(\\mathbf{x}) \\ge 0$\n",
    "2. $\\int p(\\mathbf{x})\\text{d}\\mathbf{x} = 1$\n",
    "\n",
    "The integral of the joint pdf is taken over the entire space spanned by $\\mathbf{x}$. For example, if $\\mathbf{x} = \\langle x_1, x_2 \\rangle$, then the integral is evaluated:\n",
    "$$\\int_{x_2} \\int_{x_1} p(\\mathbf{x})\\text{d}\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum and product rules, as well as Baye's theorem, apply to continuous RVs as well as to *combinations of discrete and continuous RVs*.\n",
    "- **Sum Rule:** $$p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{y}) \\text{d} \\mathbf{y}$$\n",
    "- **Product Rule:** $$p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})$$\n",
    "- Baye's Theorem: $$p(\\mathbf{y}|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathbf{y})p(\\mathbf{y})}{p(\\mathbf{x})} \\\\ \\ \\\\ p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{y})p(\\mathbf{y})\\text{d}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations and Covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectations** (in general form):\n",
    "$$E[f] = \\sum_x p(x) f(x) \\\\ \\ \\\\ E[f] = \\int p(x) f(x) \\text{d} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Marginal expectations** for joint pdfs are denoted: $E_x[f(x,y)]$\n",
    "\n",
    "**Conditional Expectations** with respect to a conditional distribution:\n",
    "$$E_x[f|y] = \\sum_x p(x|y)f(x) \\\\ \\ \\\\ E_x[f|y] = \\int p(x|y)f(x)\\text{d}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**: $$V[f] = E\\big[ \\big( f(x) - E[f(x)] \\big)^2\\big] \\\\ \\ \\\\\\implies V[f] = E[f(x)^2] - E[f(x)]^2 \\\\ \\ \\\\\\implies V[x] = E[x^2] - E[x]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance:** $$\\text{cov}[x,y] = E_{x,y}\\big[\\{x-E[x]\\}\\{y-E[y]\\}\\big] \\\\ \\ \\\\ = E_{x,y}[xy] - E[x]E[y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix of two vectors is given by:\n",
    "$$\\text{cov}[\\mathbf{x}, \\mathbf{y}] = E_{\\mathbf{x}, \\mathbf{y}} \\big[ \\{\\mathbf{x} - E[\\mathbf{x}]\\}\\{\\mathbf{y}^\\intercal - E[\\mathbf{y}^\\intercal]\\} \\big] \\\\ \\ \\\\ = E_{\\mathbf{x}, \\mathbf{y}}\\big[\\mathbf{x}\\mathbf{y}^\\intercal\\big] - E[\\mathbf{x}]E[\\mathbf{y}^\\intercal]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the covariance of a vector with itself is denoted: $$\\text{cov}[\\mathbf{x}] \\equiv \\text{cov}[\\mathbf{x}, \\mathbf{x}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will mostly skip this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a row vector $\\mathbf{X} = (x_1,...,x_N)$ representing N observations of a scalar random variable $x$. \\\n",
    "To be clear about notation, this is different from the $D$ dimensional vector-valued random variable denoted $\\mathbf{x} = (x_1, ..., x_D)^\\intercal$. That RV, $\\mathbf{x}$, spans a $D$ dimensional subspace and may be thought of as a *single observation* with $D$ features. Whereas the RV $\\mathbf{W}$ spans a single dimension and may be thought of as $N$ observations of a single feature.\\\n",
    "Anyways, let these $N$ observations be drawn from a Gaussian, $\\mathcal{N}(\\mu, \\sigma^2)$, whose parameters $\\mu$ and $\\sigma^s$ are *unknown*. We would like to determine these parameters from the data set. In this case, we are estimating *the distribution* rather than a value drawn from it. This is known as ***Density Estimation***.\\\n",
    "Because the observations in $\\mathbf{X}$ are drawn from the same distribution, they are ***Independent and Identically Distributed*** (IID). Because they are independent, their joint pdf is simply the product of their marginal probabilities. So: $$p(\\mathbf{X}|\\mu, \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n|\\mu, \\sigma^2)$$\n",
    "This is the distribution's ***Likelihood Function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we may determine the parameters $\\mu$ and $\\sigma^2$ by selecting their values that maximize this likelihood function. This is the method of ***Maximum Likelihood Estimation*** (MLE). This is practically simplified by maximizing over the log of the likelihood function since the log allows us to separate the product into a sum. We are able to solve the same maximization problem through the *log-likelihood function* because the log is a monotonically increasing function of its argument, so maximizing over the log is the same as maximizing over the argument (i.e. the likelihood function itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll skip the derivation of the actual log-likelihood function for the normal distribution, just wanted to write out the concept. However, the MLE estimates yielded are:\n",
    "$$\\hat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N x_n \\\\ \\ \\\\ \\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\hat{\\mu})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias of Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE has some limitations. Note that the MLE parameter estimates are functions of the observed data. Their expectations are: $$E[\\hat{\\mu}] = \\mu \\\\ \\ \\\\ E[\\hat{\\sigma}^2] = \\bigg(\\frac{N-1}{N}\\bigg)\\sigma^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE estimate for $\\mu$ is the true, or population, $\\mu$. However, the MLE estimate for $\\sigma^2$ is underestimated by a factor of $(N-1)/N$. This estimate is ***Biased***. Note that the bias in the MLE estimate for $\\sigma^2$ arises because this estimate is measured relative to the MLE estimate for $\\mu$ which itself is tuned to the data. If we kne the true value of $\\mu$, then we could make an unbiased estimate of $\\sigma^2$. In any case, for a Gaussian distribution, we may produce an unbiased estimate of $\\sigma^2$ by simply multiplying the MLE estimate by a factor of $N/(N-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Transformation of Probability Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition from Integration by Substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take an integral $\\int_a^b f(x) dx$. Let us apply a transformation function $y(x)$, the resulting integral from the transformation is:\n",
    "$$\\int_a^b f(x) dx \\ \\rightarrow \\ \\int_{y(a)}^{y(b)}f\\big(y^{-1}(x)\\big)\\frac{dx}{dy}dy$$\n",
    "Note that this second integrand is expressed *entirely* in terms of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let this be a CDF on $(a,b)$, so that we may replace $f(x)$ with $p_x(x)$. \\\n",
    "Then a transformation applied to $x$ via *change of variables* is $x = g(y)$. $p_x(x)$ is the density of $x$ while $p_y(y)$ is the density of $y$. The function $g(\\cdot)$ maps $y:\\rightarrow x$\\\n",
    "So, $p_x(x) = p_x(g(y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What then is $p_y(y)$? Refer to the integrand above: $$p_y(y) = p_x(g(y))\\bigg|\\frac{dg}{dy}\\bigg|$$\n",
    "Note that the absolute value of the derivative w.r.t. $y$ is taken because the derivative may be negative but the pdf must be strictly positive.\\\n",
    "Note also that, like the integrand, this is expressed wholly in terms of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Transformed Density Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important quality of density functions produced by a ***non-linear transformation*** is that their modes may not correspond to the transformed value of the mode of the original density function. Specifically, the mode of the transformed density function depends on the choice of the transformed variable.\n",
    "\n",
    "Take a monotonically increasing function $f(x)$. Then the maximum value of the function is found at $\\hat{x}$ as $f(\\hat{x})$. This global maximum may be found by taking the derivative of the function w.r.t. $x$ and setting it equal to 0; i.e. at $\\hat{x}$ s.t. $f'(\\hat{x})=0$. Now, let's replace $x$ with $x=g(y)$. Then, the correspond maximum occurs at $\\hat{y}$ s.t. $f'(g(\\hat{y}))g'(\\hat{y}) = 0$. Thus, $\\hat{x} = g(\\hat{y})$ as our intuition would suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let $\\hat{x}$ be the mode of the density function $p_x(x)$. Note that the mode of a density function corresponds to its maximum. (This is more intuitive for discrete RVs where the mode is simply the most often occuring value of the RV). Then, transforming $x=g(y)$ yields the density function: $$p_y(y) = p_x(g(y))|g'(y)|$$\n",
    "Taking the first derivative w.r.t. $y$ yields: $$p'_y(y) = |p'_x(g(y))[g'(y)]^2| + |p_x(g(y))g''(y)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the second term, the relationship $\\hat{x} = g(\\hat{y})$ no longer holds. The book withholds this proof and it looks like a pain to derive so I'll take it at face value. However, it should be noted that for a **linear transformation** the relationshp $\\hat{x} = g(\\hat{y})$ does hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider once more a $D$ dimensional variable $\\mathbf{x} = (x_1,...,x_D)^\\intercal$. Suppose that we transform it to a new variable $\\mathbf{y} = (y_1, ..., y_D)^\\intercal$, where $\\mathbf{x} = g(\\mathbf{y})$. Then the transformed density is given by: $$p_{\\mathbf{y}}(\\mathbf{y}) = p_{\\mathbf{x}}(\\mathbf{x})|\\text{det}\\mathbf{J}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\mathbf{J}$ is the ***Jacobian Matrix** of $\\mathbf{y}$. Its elements are the partial derivatives $J_{ij} = \\partial g_i/ \\partial y_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multivariable calculaus, the determinant of the Jacobian Matrix may be interpreted as the scaling factor by which the area of a surface spanned by multiple variables (e.g. $x$ and $y$) changes when the multivariate function is transformed from being expressed in its original space (e.g. the $(x,y)$ coordinates) to being expressed in the new, transformed space (e.g. $(u,v)$ coordinates). \\\n",
    "We may apply a similar interpretation in this context of probability densities. The change of variables changes the subspace spanned by the probability density function. The absolute value of the determinant of the Jacobian matrix represents the ratio of the volume of the transformed subspace to the volume of the original subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy may be thought of as a measure of *surprise*. That is, the surprise elicited by an event's occurance. For example, if we observe a $1$ from a roll of a fair die we would not be surprised at all. However, if somebody predicted that the roll will result in a $1$ then we may be surprised. If they called the next three rolls correctly then we may be about three timmes as surprised. The function that represents surprise should then be decreasing in probability (i.e. more surprise for lower probability events), and should be additive (i.e. linearly increasing surprise as more surprising events occur). Such a function is: $$h(x) = \\log \\frac{1}{p(x)} \\equiv -\\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about surprise is as *information content* or *information gain*. So, the cumulative suprise of observing multiple events is equivalently the cumulative information gain.\n",
    "\n",
    "The ***Entropy*** of a distribution is the average information content (surprise) of the distribution: $$H[x] = -\\sum_x p(x) \\log p(x)$$ If we choose to use $\\log_2$, then the entropy of a process may be expressed in bits. Although the choice of basis for the logarithm is truly arbitrary, it is useful to express entropy as bits of information.\n",
    "\n",
    "For example, consider an RV $x$ with eight possible states, each of which is equally likely. This is a Bernouli RV with $p(x) = 1/8$. What is they entropy of this random variable?\n",
    "$$H[x] = -8 \\bigg(\\frac{1}{8} \\log_2 \\frac{1}{8}\\bigg) = 3$$\n",
    "So, 3 bits. Now consider a non-uniform RV with 8 possible states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "states = [1/2, 1/4, 1/8, 1/16] + [1/64]*4\n",
    "entropy = sum(-state * np.log2(state) for state in states)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-uniform RV has a *lower* entropy than the uniform RV. The intuition for this behavior is that the non-uniform distribution, despite having several less likely outcomes also has several more likely outcomes (i.e. $p(x_1) = 1/2$ and $p(x_2) = 1/4$). So, the average information content in the distribution is lower. Equivalently, we are less surprised by outcomes drawn from this distribution *more* of the time.\n",
    "\n",
    "Another, more comprehensive specification of entropy is: The average amount of information needed to specify the state of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous RVs, entropy is expressed as: $$H[x] = -\\int p(x)\\ln p(x)dx$$\n",
    "For a desnity defined over multiple continuous RVs, denoted collectively by the vector $\\mathbf{x}$, the differential entropy is given by: $$H[\\mathbf{x}] = -\\int p(\\mathbf{x}) \\ln p(\\mathbf{x}) d\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that the maximum entropy distribution for a discrete RV corresponds to a uniform distribution across the possible states of the variable. For a continuous RV it turns out that the maximum entropy distribution is a Gaussian distribution. I'm not going to step trhough the textbook's logic to demonstrate this, but intuitively it is a bit surprising. Entropy increases as distributions become more diffuse, so the uniform distribution maximizing entropy for discrete RVs makes sense. The Gaussian doesn't seem like the most diffuse continuous distribution. Indeed, the uniform distribution can be continuous as well. So, it's noteworthy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an unknown distribution $p(x)$ which we are modeling with an approximating distribution $q(x)$. The KL divergence measures the *average additional amount* of information required to specify the value of $x$ as a result of using $q(x)$ instead of $p(x)$. It is given by: $$KL(p||q) = -\\int p(x)\\ln q(x) dx - \\bigg(-\\int p(x) \\ln p(x) dx \\bigg) \\\\ \\ \\\\ = -\\int p(x) \\ln \\bigg(\\frac{q(x)}{p(x)}\\bigg)dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term includes $p(x)$ because the true probability of the event $x=x_i$ is given by $p(x)$.\\\n",
    "Note that this is the difference in the observed entropy of the modeled distribution $q(x)$ and the true entropy of the unknown distribution $p(x)$. KL divergence is sometimes also called *relative entropy* for this reason. Note also that the quantity is not symmetrical: $KL(p||q) \\ne KL(q||p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jensen's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jensen's Inequality states that any convex function $f(x)$ satisfies the inequality: $$f\\bigg(\\sum_{i=1}^M \\lambda_i x_i \\bigg) \\le \\sum_{i=1}^M \\lambda_i f(x_i)$$ where $\\lambda_i \\ge 0$ and $\\sum_i \\lambda_i = 1, \\ \\forall \\ x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_i$ may be interpreted as the *probability distribution* over a discrete RV $x$: $$f(E[x]) \\le E[f(x)]$$\n",
    "Then, for a continuous RV it may be expressed as: $$f\\bigg(\\int xp(x)dx \\bigg) \\le \\int f(x)p(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Jensen's inequality applied to KL divergence yields: $$KL(p||q) = -\\ln \\int q(x)dx \\le - \\int p(x) \\ln \\bigg(\\frac{q(x)}{p(x)}\\bigg)$$\n",
    "From the normalization condition for a probability distribution: $\\int q(x)dx = 1$, so the left side of this inequality is equal to 0. Therefore, for the KL divergence to be convex, it must be true that $q(x) = p(x) \\ \\forall \\ x$. Thus, KL divergence may be interpreted as a measure of dissimilarity betweent the two distributions... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to divergence, we may think of cross-entropy as quanitying the average surpise (i.e. information gain) from observing a random variable governed by an unknown distribution $P$, while believing it is governed by a modeled distribution $Q$. The cross-entropy is expressed as: $$H(P,Q) = -\\sum p(x) \\ln q(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the discrete variable analogue to the first term in the KL divergence.\\\n",
    "A key property of cross-entropy is that $$H(P,Q) \\ge H(P), \\ \\forall \\ P, Q$$\n",
    "In other words, believing in a model of an unknown process can never *decrease* the entropy (or surprisal) of observations drawn from that process.\\\n",
    "Also, note that like KL divergence, cross-entropy is assymetric: $$H(P,Q) \\ne H(Q,P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in a modeling task, it is often rational to seek to minimize the KL divergence in between the model trained and the observed distribution in the training data. Within the context of deep learning, minimizing with respect to the KL divergence is the same as minimizing with respect to the cross entropy, because the cross entropy is the term of the KL divergence that includes $Q$, the modeled distribution including the trainable parameters. So, selecting parameters of $Q$ to minimize cross-entropy is the same as selecting parameters of $Q$ to minimize KL divergence. Hence, cross-entropy loss is used over KL divergence loss.\n",
    "\n",
    "Similarly, suppose we are approximating the unknown distribution using a parametric distribution $q(x|\\theta)$ governed by a set of adjustable parameteres $\\theta$. We may determine the optimal value of $\\theta$ by minimizing the KL divergence between $q(x|\\theta)$ and $p(x)$. However, we do not know $p(x)$, so the best we can do is approximate the KL divergence using a finite set of training points $x_n$ drawn from a sample of $p(x)$ of size $N$: $$KL(p||q) \\approxeq \\frac{1}{N}\\sum_{n=1}^N \\big(-\\ln q(x_n|\\theta) + \\ln p(x_n) \\big)$$ Because $\\ln p(x_n)$ is independent of $\\theta$, minimizing the KL divergence w.r.t. $\\theta$ is equivalent to minimizing the conditional negative log-likelihood function: $$-\\frac{1}{N}\\sum_{n=1}^N - \\ln q(x_n|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a joint distribution $p(x,y)$. If a value of $x$ is already known, then the conditional surprise for an observed value of $y$ is $-\\ln p(y|x)$. Thus, the conditional entropy of the joint process is: $$H[y|x] = -\\int \\int p(y,x)\\ln p(y|x) dydx$$ \n",
    "This is called the *conditional entropy* of $y$ given $x$.\\\n",
    "By the product rule, it may be shown that: $$H[x,y] = H[y|x] + H[x]$$\n",
    "So, the information needed to specify $x$ and $y$ together is equal to the information needed to specify $x$ on its own, *plus* the information needed to specify $y$ given $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two random variables *are not* independent, we may use KL divergence to determine how *close* they are to being independent by considering the KL divergence between their joint distribution and the product of their marginal distributions. The intuition for this comes from the fact that a joint distribution for independent $x$ and $y$ is given by $p(x,y) = p(x)p(y)$.\n",
    "$$I[x,y] \\equiv KL\\big(p(x,y)||p(x)p(y)\\big) \\\\ \\ \\\\ = - \\int \\int p(x,y)\\ln \\bigg( \\frac{p(x)p(y)}{p(x,y)} \\bigg)dxdy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be related to the conditional entropy of $x$ and $y$ as: $$I[x,y] = H[x] - H[x|y] = H[y] - H[y|x]$$\n",
    "Thus, the mutual information represents the reduction in the uncertainty about $x$ by being told the value of $y$ (or vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{D}$ denote the training data set and $\\mathbf{w}$ denot a set of parameter values (or weights). The likelihood function $p(\\mathcal{D}|\\mathbf{w})$ expresses the likelihood (or probability) of observing the training data given the parameter values. Selecting optimal parameter values then corresponds to choosing $\\mathbf{w}$ such that $p(\\mathcal{D}|\\mathbf{w})$ is maximized. This is equivalent to minimizing over the negative logarithm of the likelihood function. In ML literature, the negative log-likelihood function is call an *error function*.\n",
    "\n",
    "The prior distribution of $\\mathbf{w}$ is $p(\\mathbf{w})$. This may be interpreted as our choice of parameters before observing any training data. Then, the posterior distribution of the parameter values is related by Baye's Theorem: $$p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w}) p(\\mathbf{w})}{p(\\mathcal{D})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Bayseian paradigm, there is only a single dataset $\\mathcal{D}$, and the uncertainty in the parameters is expressed through a probability distribution over $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of choosing the model parameteres by maximizing the likelihood function w.r.t. $\\mathbf{w}$, we can maximize the *posterior probabilities*. This is of course equivalent to minimizing the negative log posterior probability: $$-\\ln p(\\mathbf{w}|\\mathcal{D}) = - \\ln p(\\mathcal{D}|\\mathbf{w}) - \\ln p(\\mathbf{w}) + \\ln p(\\mathcal{D})$$\n",
    "This is called the ***Maximum a Posteriori Estimate*** or MAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding $\\ln p(\\mathcal{D})$ because it is independent of $\\mathbf{w}$, we may recognize that this is a regularized cost function where $-\\ln p(\\mathbf{w})$ is the regularization penalty term applied to the MLE negative log-likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, given the training data set $\\mathcal{D}$ and an input value $x$, we are interested in predicting a target value $t$. From the sum and product rules of probability:\n",
    "$$p(t|x,\\mathcal{D}) = \\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is effectively a weighted average of all conditional probabilities of the target variable (conditioned on the input and parameter weights) where the weights are given by the *posterior* probability distribution $p(\\mathbf{w}|\\mathcal{D})$. This is the key difference that distinguishes Bayesian methods - they integrate over the space of parameters, whereas frequentist methods use point estimates for parameters that are obtained by optimizing a loss function (like sum-of-squares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a maximum likelihood approach to polynomial regression would select the model that gives the highest probability of the observed data (this is how parameters are selected through MLE). However, a Bayesian approach would average over all possible models, witht the contribution of each being weighted by its posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "Given an RV $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$ where $\\mathbf{u} \\sim p_{\\mathbf{u}}(\\mathbf{u})$ and $\\mathbf{v} \\sim p_{\\mathbf{v}}(\\mathbf{v})$. Show that, $$p_{\\mathbf{y}}(\\mathbf{y}) = \\int p_{\\mathbf{u}}(\\mathbf{u}) p_{\\mathbf{v}}(\\mathbf{v} - \\mathbf{u})d\\mathbf{u}$$ Where $\\mathbf{u}$ and $\\mathbf{v}$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
