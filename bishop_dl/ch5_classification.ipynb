{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Distinct Approaches to Classification:**\n",
    "1. Use a ***Discriminant Function*** to directly assign each imput vector $\\bf x$ to a specific class\n",
    "2. Determine the class-specific conditional distributions $p(C_k|\\bf x)$ through parametric modeling, then use the distributions to make optimal classification decisions by optimizing over the parametric model(s)\n",
    "    - This is ***Discriminative Probabilistic Modeling***\n",
    "3. Alternatively, model the class-conditional densities $p(\\mathbf{x}|C_k)$ along with the prior class probabilities $p(C_k)$, then compute the posterior probabilities via Baye's Theorem: $$p(C_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|C_k) p(C_k)}{p(\\mathbf{x})}$$\n",
    "    - This is ***Generative Probabilistic Modeling***, so called because it offers the opportunity to generate samples from each of the class-conditional densities $p(\\mathbf{x}|C_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest representation of a linear discriminant function is:\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^\\intercal \\mathbf{x} + w_0$$\n",
    "When modeling a binary class system, the input vector is assigned to class $C_1$ if $y(\\mathbf{x}) \\ge 0$ and to class $C_2$ otherwise. This *linear* discriminant function creates a decision boundary that is a $(D-1)$-dimensional hyperplane within the $D$-dimensional space of the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality of $\\bf w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$, both of which lie upon the decision surface. Then, for these points we have:\n",
    "$$y(\\mathbf{x}_A) = y(\\mathbf{x}_B) = 0 \\ \\rightarrow \\ \\mathbf{w}^\\intercal (\\mathbf{x}_A - \\mathbf{x}_B) = 0$$\n",
    "Thus, $\\mathbf{w}$ is orthogonal to every vector upon the decision surface. Why?\\\n",
    "Recall that the dot-product between two vectors $\\bf w$ and $\\bf x$ equals: $$\\|\\mathbf{w}\\|\\|\\mathbf{x}\\|\\cos(\\theta)$$ Where $\\theta$ is the angle between the vectors. In words, this is the cosine simialrity between the vectors' directions scaled by the product of their magnitudes. Then, for the dot product of the two vectors to be $0$, one of two things must be true:\n",
    "1. One of the vectors has zero magnitude (will not be the case for all $\\mathbf{x}_A$ and $\\mathbf{x}_B$ and is not the case for $\\mathbf{w}$ because it defines the discriminant function)\n",
    "2. The cosine of the angle between the vectors is zero, which means that the vectors are perpendicular: θ = 90°\n",
    "Because $\\mathbf{x}_A - \\mathbf{x}_B$ is a vector lying upon the decision surface, $\\mathbf{w}$ must be perpendicular to the decision surface.\n",
    "\n",
    "So, $\\bf w$ is orthogonal to the hyperplane that is the decision boundary. This means that it is the ***Normal Vector*** that *defines* the hyperplane's direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the distance from the origin to the decision surface may be given by: $$\\frac{\\mathbf{w}^\\intercal \\mathbf{x}}{\\| \\mathbf{w} \\|} = -\\frac{w_0}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "So, the bias parameter $w_0$ determines the location of the decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Claude Explanation:*\n",
    "> We've just learned that w is perpendicular to the decision surface. Therefore, the shortest path from the origin to the surface must lie along the direction of w (or -w). Let's call the point where this perpendicular line meets the surface x*.\\\n",
    "Here's where we can use a clever trick: we know that x* must be some scalar multiple of w (since it lies along w's direction). Let's call this scalar α:\\\n",
    "x* = αw\\\n",
    "Since x* lies on the decision surface, it must satisfy our original equation:\\\n",
    "w^T(αw) + w₀ = 0\\\n",
    "Using the properties of transposes:\\\n",
    "αw^Tw + w₀ = 0\\\n",
    "Note that w^Tw is just ||w||², the squared magnitude of w. So:\\\n",
    "α||w||² + w₀ = 0\\\n",
    "Solving for α:\\\n",
    "α = -w₀/||w||²\\\n",
    "Now, x* = αw = (-w₀/||w||²)w is our point on the surface. The distance we're looking for is ||x*||:\\\n",
    "||x*|| = ||(-w₀/||w||²)w|| = |-w₀/||w||²| · ||w|| = |w₀|/||w||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the distance of any point $\\bf x$ to the decision surface is simply the distance between $\\bf x$ and its orthogonal projection upon the decision surface $\\mathbf{x}_\\perp$. We may express $\\bf x$ in terms of $\\mathbf{x}_\\perp$ as: $$\\mathbf{x} = \\mathbf{x}_\\perp + r \\frac{\\mathbf{w}}{\\| \\mathbf{w} \\|}$$\n",
    "That is, $\\mathbf{x}$ is its orthogonal projection upon the decision surface $\\mathbf{x}_\\perp$ plus the distance to its orthogonal projection. $r$ is the signed perpendicular distance  and is given by: $$r = \\frac{y(\\mathbf{x})}{\\|\\mathbf{w}\\|}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
