{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive expressions for the elements of the $2\\times 2$ Hessian matrix w.r.t. the weight and bias parameters of a linear regression model with the following form and error function:\n",
    "$$\n",
    "y(x, w, b) = wx + b \\\\ \\ \\\\\n",
    "E(w, b) = \\frac{1}{2}\\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\}^2\n",
    "$$\n",
    "Then show that the trace and determinant of this Hessian are both positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian\n",
    "The Hessian is the matrix of second partial derivatives w.r.t. the parameters $w$ and $b$. So, let's begin by taking the first partial derivatives. We may do this directly with matrix calculus but they are easy to derive without it:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} = \\frac{1}{2} \\sum_{n=1}^N \\frac{\\partial}{\\partial w} \\{y(x_n, w, b) - t_n \\}^2 \\\\ \\ \\\\\n",
    "= \\frac{1}{2} \\sum_{n=1}^N \\frac{\\partial}{\\partial y}\\{y(x_n, w, b) - t_n \\}^2 \\frac{\\partial y}{\\partial w} \\\\ \\ \\\\\n",
    "= \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} x_n \\\\ \\ \\\\\n",
    "= \\bf x^\\intercal ( y - t ) \n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b} = \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} = \\bf 1^\\intercal ( y - t) \n",
    "$$\n",
    "Where $\\bf 1$ is a vector of $1$'s with length $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the second partial derivatives:\n",
    "$$\n",
    "f_{ww} = \\frac{\\partial^2 E}{\\partial w^2} = \\frac{\\partial}{\\partial w} \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} x_n \\\\ \\ \\\\\n",
    "= \\sum_{n=1}^N x_n^2 =  \\mathbf{x}^\\intercal \\mathbf{x} \\\\ \\ \\\\\n",
    "f_{bb} = \\frac{\\partial^2 E}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} \\\\ \\ \\\\\n",
    "= \\sum_{n=1}^N 1 = N \\\\ \\ \\\\\n",
    "f_{wb} = \\frac{\\partial^2 E}{\\partial w \\partial b} = \\frac{\\partial}{\\partial b} \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} x_n \\\\ \\ \\\\\n",
    "= \\sum_{n=1}^N x_n = \\mathbf{x}^\\intercal \\mathbf{1} \\\\ \\ \\\\\n",
    "f_{bw} \\frac{\\partial^2 E}{\\partial b \\partial w} = \\frac{\\partial}{\\partial w} \\sum_{n=1}^N \\{y(x_n, w, b) - t_n \\} \\\\ \\ \\\\\n",
    "= \\sum_{n=1}^N x_n = \\mathbf{x}^\\intercal \\mathbf{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the Hessian is:\n",
    "$$\n",
    "\\mathbf{H} = \n",
    "\\begin{bmatrix}\n",
    "f_{ww} \\ f_{wb} \\\\\n",
    "f_{bw} \\ f_{bb}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\sum x_n^2 \\ \\sum x_n \\\\\n",
    "\\sum x_n \\ \\ \\ \\ \\ N\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}^\\intercal \\mathbf{x} \\ \\ \\ \\mathbf{x}^\\intercal \\mathbf{1} \\\\\n",
    "\\mathbf{x}^\\intercal \\mathbf{1} \\ \\ \\ \\ \\ N\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinant\n",
    "$$\n",
    "\\text{det}(\\mathbf{H}) = f_{ww}f_{bb} - f_{bw}f{wb} = N \\mathbf{x}^\\intercal \\mathbf{x} - \\mathbf{x}^\\intercal \\mathbf{1} \\mathbf{x}^\\intercal \\mathbf{1} \\\\ \\ \\\\\n",
    "= N\\sum x_n^2 - \\big(\\sum x_n \\big)^2\n",
    "$$\n",
    "This is $N$ times the sum of squares of $\\bf x$ minus the squared sum of $\\bf x$. This may remind us of the variance:\n",
    "$$\n",
    "\\mathbb{V}(\\mathbf{x}) = \\mathbb{E}\\big[\\mathbf{x}^2\\big] - \\mathbb{E}[\\mathbf{x}]^2 = \\frac{1}{N}\\sum_{n=1}^N x_n^2 - \\bigg(\\frac{1}{N}\\sum_{n=1}^N x_n\\bigg)^2\n",
    "\\\\ \\ \\\\\n",
    "\\implies \\text{det}(\\mathbf{H}) = N^2 * \\mathbb{V}(\\mathbf{x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the determinant is:\n",
    "- Always non-negative since the variance is non-negative\n",
    "- Zero *only* when all observations of the input variable $x_n$ are identical\n",
    "- Increasing in the variance\n",
    "    - And therefore, increasing in the spread of $x_n$ realizations and the magnitude of the $x_n$ values\n",
    "\n",
    "The determinant may be geometrically interpreted as the \"volume\" of a tranformation resulting from a matrix. In this case, the volume of the transformation represented by the Hessian. As the Hessian is comprised of second partial derivatives w.r.t. the parameters, it may be thought of as describing how the gradient (i.e. the first partial derivative) changes in repsonse to a change in the parameters. A *larger* determinant, therefore, suggests a larger change in the gradients due to a change in the parameters. The magnitude of this change is dictated by the variance in the input variable $\\bf x$ and the number of input observations $N$. When $\\mathbb{V}(\\bf x)$ is large, the determinant is large, indicating a steeper increase in the gradients w.r.t. changes in the parameters $w$ and $b$. This suggests that the parameters $w$ and $b$ have greater influence on the predictions. When variance is low, this influence is weaker. In the case of constant $x_n$, such that $\\mathbb{V}(\\bf x) = 0$, the determinant will be $0$, and thus the parameters $w$ and $b$ will ahve *no effect* of the loss function, and therefore no effect on the predictions. In this case, $w$ and $b$ are perfectly linearly dependent - many combinations of $w$ and $b$ can give the same predictions.\n",
    "\n",
    "So, a larger determinant indicates that the curvature of the error surface is more pronounced (steeper); indicating that the optimization problem is more well conditioned.\n",
    "\n",
    "Because the determinant is positive, the critical points must be either local maxima or minima.\n",
    "\n",
    "The determinant of a matrix is **equal to the product of its eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace\n",
    "The trace of a square matrix is the sum of the elements on its main diagonal:\n",
    "$$\\text{tr}(\\mathbf{A}) = a_{11} + a_{22} + \\cdots + a_{nn}$$\n",
    "The trace of a matrix is **equal to the sum of its eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tr}\\mathbf{H} = f_{ww} + f_{bb} = \\sum_{n=1}^N x_n^2 + N$$\n",
    "This is strictly positive, thus the sum of the Hessian's eigenvalues is positive.\n",
    "\n",
    "Since the sum and product of the Hessian's eigenvalues are both positive, the crtical point of the error function **must be a minimum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9\n",
    "This exercis asks us to derive some of the expressions given in the text. We are given the following expressions for weights initialized by a Gaussian distribution with $\\mathcal{N}(0, \\epsilon^2)$:\n",
    "$$\n",
    "a_i^{(l)} = \\sum_{j=1}^M w_{ij}z_j^{(l-1)} \\\\ \\ \\\\\n",
    "z_i^{(l)} = \\text{ReLU}\\big(a_i^{(l)}\\big)\n",
    "$$\n",
    "Where $z_j^{(l-1)}$ has variance $\\lambda^2$. \n",
    "\n",
    "Then we are asked to show that:\n",
    "$$\n",
    "\\mathbb{E}\\big[a_i^{(l)}\\big] = 0 \\\\ \\ \\\\\n",
    "\\mathbb{V}\\big[a_i^{(l)}\\big] = \\frac{M}{2}\\epsilon^2\\lambda^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\big[a_i\\big] = \\sum_{j=1}^M \\mathbb{E} \\big[w_{ij}z_j^{(l-1)}\\big] \n",
    "$$\n",
    "Assuming that the weights are independent of the activations from the previous layer (i.e. $z_j^{(l-1)}$), which is typically the case, then this expression is $0$ because the weights of layer $l$ are initialized by a zero-centered Gaussian:\n",
    "$$\n",
    "\\mathbb{E}\\big[a_i\\big] = \\sum_{j=1}^M \\mathbb{E} \\big[w_{ij}z_j^{(l-1)}\\big] = \\sum_{j=1}^M 0 \\mathbb{E}\\big[z_j^{(l-1)}\\big] = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the variance:\n",
    "$$\n",
    "\\mathbb{V}\\big[a_i^{(l)}\\big] = \\mathbb{E}\\big[\\big(a_i^{(l)}\\big)^2\\big] - \\big(\\mathbb{E}\\big[a_i^{(l)}\\big]\\big)^2 \\\\ \\ \\\\\n",
    "= \\sum_{j=1}^M \\mathbb{E} \\bigg[\\big(w_{ij}z_j^{(l-1)}\\big)^2\\bigg] - 0^2 \\\\ \\ \\\\\n",
    "= \\sum_{j=1}^M \\mathbb{E} \\bigg[w_{ij}^2 \\big(z_j^{(l-1)}\\big)^2\\bigg]\n",
    "$$\n",
    "By independence of weights and prior activations:\n",
    "$$\n",
    "\\sum_{j=1}^M \\mathbb{E} \\bigg[w_{ij}^2 \\big(z_j^{(l-1)}\\big)^2\\bigg] = \\sum_{j=1}^M \\mathbb{E}\\bigg[w_{ij}^2\\bigg] * \\mathbb{E}\\bigg[\\big(z_j^{(l-1)}\\big)^2\\bigg] \\\\ \\ \\\\\n",
    "= \\sum_{j=1}^M \\bigg(\\mathbb{V}[w_{ij}] - 0 \\bigg) * \\mathbb{E}\\bigg[\\big(z_j^{(l-1)}\\big)^2\\bigg] \\\\ \\ \\\\\n",
    "= \\sum_{j=1}^M \\epsilon^2 * \\mathbb{E}\\bigg[\\big(z_j^{(l-1)}\\big)^2\\bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the expectation of the squared prior activations, which are ReLU activated preactivations. If the weights of these preactivations are also zero-centered Gaussian (which may be inferred but isn't clear from the prompt), then the *distribution of the preactivations is symmetric* and half of the activations will be $0$. Thus, we may express:\n",
    "$$\n",
    "\\mathbb{E}\\big[\\big(z_j^{(l-1)}\\big)^2\\big] = \\frac{1}{2} \\mathbb{E}\\big[\\big(a_i^{(l-1)}\\big)^2\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the zero-mean Gaussian, the expected value of the preactivations is $0$, so:\n",
    "$$\n",
    "\\mathbb{V}\\big[z_j^{(l-1)}\\big] = \\frac{1}{2}\\mathbb{E}\\big[\\big(a_i^{(l-1)}\\big)^2\\big] - \\frac{1}{4}\\mathbb{E}\\big[a_i^{(l-1)}\\big]^2 \\\\ \\ \\\\\n",
    "= \\frac{1}{2}\\mathbb{E}\\big[\\big(a_i^{(l-1)}\\big)^2\\big] - 0 \\\\ \\ \\\\\n",
    "= \\frac{1}{2}\\lambda^2 = \\mathbb{E}\\big[\\big(z_j^{(l-1)}\\big)^2\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "$$\n",
    "\\mathbb{V}\\big[a_i^{(l)}\\big] = \\sum_{j=1}^M \\epsilon^2 * \\mathbb{E}\\bigg[\\big(z_j^{(l-1)}\\big)^2\\bigg] = \\frac{M}{2}\\epsilon^2\\lambda^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure that $\\mathbb{V}[z_i^{(l)}]=\\lambda^2$, we would thus need to set $\\epsilon = \\sqrt{\\frac{2}{M}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
