{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5c7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34bbc79",
   "metadata": {},
   "source": [
    "# 9.1 Inductive Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdda50",
   "metadata": {},
   "source": [
    "### 9.1.1 Inverse Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b1b2c",
   "metadata": {},
   "source": [
    "Most machine learning problems are *inverse problems*:\\\n",
    "Given a conditional distribution, we may easily generate samples of observations from the distribution. In ML, we almost always care to solve the *inverse* of this problem - i.e. given a sample of observations, determine the distribution that generated them. The fundamental difficulty with this is that infinitely many distributions could have generated any sample of observations. \n",
    "\n",
    "The preference for one distribution over others is called *inductive bias*. This umbrella term is lent to many flavors of useful bias such as domain knowledge or medium knowledge (like knowing that the medium of images possesses translation invariance), among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44906b7",
   "metadata": {},
   "source": [
    "### 9.1.2 No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27d871",
   "metadata": {},
   "source": [
    "This theorem states that every learning algorithm is as good as any other when averaged over all possible problems...\n",
    "\n",
    "In finding good inductive biases, we want biases that are appropriate to the broad class of problems that our models will be applied to in practice. For a given application, better results are obtained by incorporating stronger inductive biases for the specific applications of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df00f4",
   "metadata": {},
   "source": [
    "### 9.1.3 Symmetry and Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4f892",
   "metadata": {},
   "source": [
    "In many applications, good predictions are *invarian* under one or more transformations of the input variables. For instance, computer vision models should be able to identify objects regardless of where they appear in an image (*translation invariance*). \n",
    "\n",
    "Transformations that leave crucial properties unchanged represent *symmetries* in an ML context. For instance, a cat should be identifiable as a cat regardless of whether it is changed by a transformation of scale or position in an image. Thus, the dominant features that identify it as a cat are *symmetric* under these transformations. The set of all transformations that correspond to a particular symmetry comprise a *group*...\n",
    "\n",
    "A *group* consists of a set of elements $A, B, C, ...$ together with a *binary operation* for compsing pairs of elements, denoted $A \\circ B$.\\\n",
    "Groups have the following axioms:\n",
    "1. **Closed** - Groups are closed under their binary operation: $$A\\circ B \\in \\mathcal{G}, \\ \\forall A, B \\in \\mathcal{G}$$\n",
    "2. **Associative**: $$(A\\circ B) \\circ C = A\\circ (B \\circ C), \\ \\forall A, B, C \\in \\mathcal{G}$$\n",
    "3. **Identity**: $$\\exists I \\in \\mathcal{G} : A \\circ I = I \\circ A = A, \\ \\forall A \\in \\mathcal{G}$$\n",
    "4. **Inverse**: $$\\exists A^{-1} \\in \\mathcal{G} : A\\circ A^{-1} = A^{-1} \\circ A = I, \\ \\forall A \\in \\mathcal{G}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae694ae",
   "metadata": {},
   "source": [
    "### 9.1.4 Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ab6bc",
   "metadata": {},
   "source": [
    "This is a generalization of invariance in which the output of the network is itself transformed when the inputs are transformed.\\\n",
    "E.g. a network that classifies pixels in an image as either being in the foreground or the background should translate the segmentations in the same way that the input is translated. \n",
    "\n",
    "Letting $\\bf I$ be an input, $S$ be the network operation, and $T$ be the transformation operation, then equivariance holds if:\n",
    "$$S(T(\\mathbf I)) = T(S(\\mathbf I))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76106ceb",
   "metadata": {},
   "source": [
    "More generally, equivariance may still apply if a different operation $\\tilde{T}$ is applied to the output such that $S(T(\\mathbf I)) = \\tilde{T}(S(\\mathbf I))$\\\n",
    "(Does $\\tilde{T}$ need to be isomorphic to $T$ or something??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af89f2",
   "metadata": {},
   "source": [
    "Invariance is a special case of equivariance in which $\\tilde{T}$ is the identity operator, i.e. : $$S(T(\\mathbf{I})) = S(\\mathbf{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff29958",
   "metadata": {},
   "source": [
    "# 9.2 Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3bdc7e",
   "metadata": {},
   "source": [
    "For a weight vector $w$ and error function $E(w)$, the $l2$ regularized error is given by: $$\\tilde{E}(w) = E(w) + \\frac \\lambda 2 w^\\intercal w$$\n",
    "Here the coefficient $1/2$ is included for convenience in differentiation.\\\n",
    "This type of regularization is called *weight decay* because it encourages weight values to decay towards zero. This principal should apply to $l1$ regularization as well, though perhaps more pronouncedly... The derivative is then:\n",
    "$$\\nabla \\tilde E (w) = \\nabla E(w) + \\lambda w$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31363ce",
   "metadata": {},
   "source": [
    "## 9.2.1 Consistent Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c82f7d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
