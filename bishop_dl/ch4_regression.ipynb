{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text treats linear regression a bit differently as a single-layer NN. So, this notebook is just going to make note of the more novel components of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Functions\n",
    "We denote input variables as $\\mathbf{x}$ and features as *basis functions* $\\mathbf{\\phi(\\mathbf{x})}$. This facilitates clearer conversation around the choices of nonlinear forms for $\\bf{x}$. Additionally, coeeficient estimates are denoted $w$ instead of $\\Beta$, which is more consistent notation with the weights used in mulit-layered neural networks. So, the predicted response variable $\\hat{t}$ is given by:\n",
    "$$\\hat{t} = y(\\mathbf{x}, \\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(\\mathbf{x}) = \\bf w^\\intercal \\phi(x)$$\n",
    "With an intercept (aka. bias) term $w_0\\phi_0(\\mathbf{x})$ for which $\\phi_0(\\mathbf{x})=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum-of-squares error function is then expressed as:\n",
    "$$E_D(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N [t_n - \\mathbf{w}^\\intercal \\mathbf{\\phi}(\\mathbf{x}_n)]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the maximum-likelihood estimation for the weights matrix is the familiar Moore-Penrose Pseudo-Inverse derived under OLS, just with design matrices expressed in basis functions:\n",
    "$$\\mathbf{w}_{ML} = \\bf (\\Phi^\\intercal \\Phi)^{-1} \\Phi^\\intercal t$$\n",
    "Where $\\bf \\Phi = \\phi(X)$ has the shape $N \\times M$ where $N$ is the number of observations and $M$ is the number of input variables (i.e. the columns in $\\bf X$, more precisely the row-space of $\\bf X$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the familiar maximum-likelihood estimator for the variance is: $$\\sigma^2_{ML} = \\frac{1}{N} \\sum_{n=1}^N [t_n - \\mathbf{w}_{ML}^\\intercal \\mathbf{\\phi}(\\mathbf{x}_n)]^2 = \\frac{2}{N} E_D(\\mathbf{w}_{ML})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Outputs\n",
    "This is a novel discussion to me. The text extends the OLS regression problem to predict multiple target variables without fitting multiple regression curves independently.\n",
    "\n",
    "Let $\\mathbf{t} = (t_1, ..., t_K)^\\intercal$. If we use the same set of basis functions for the prediction of each $t_k \\in \\mathbf{t}$, then we may predict $\\hat{\\mathbf{t}}$ as:\n",
    "$$\\bf \\hat{t} = y(x, w) = W^\\intercal \\phi(x)$$\n",
    "Where $\\bf{W}$ is an $M\\times K$ matrix of weights column vectors associated with each of the $K$ target variables, and $\\bf \\phi(x)$ is an $M$-dimensional column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum-likelihood estimator for $\\bf W$ is: $$\\mathbf{W}_{ML} = \\bf (\\Phi^\\intercal \\Phi)^{-1}\\Phi^\\intercal T$$\n",
    "Which is the same as the individual case, just substitute in the $N\\times K$ matrix $\\bf T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normally-distributed random errors $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, the predicted response $\\hat{t} = y(\\bf x, w)$ is the $\\bf x$-dependent mean of the conditional random variable $t|\\bf x$. Thus, the *predictive distribution* of this random variable (i.e. its conditional probability density) is given by:\n",
    "$$p(t|\\mathbf{x}, \\mathbf{w}_{ML}, \\sigma^2_{ML}) = \\mathcal{N}(t|y(\\mathbf{x}, \\mathbf{w}_{ML}), \\sigma^2_{ML})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider a more rigorous framework for determining the optimal value of the predicted response variable, which will be denoted $\\hat{t}^* = f(\\bf{x})$. When this value is taken to be the conditional mean, then $\\hat{t}^* = \\hat{t} = y(\\mathbf{x}, \\mathbf{w}_{ML})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error associated with choosing a predicted value $f(\\mathbf{x})$ is determined by the loss function $L(t, f(\\mathbf{x}))$. When the true value of $t$ is unknown, as is almost always the case because what we actually observe is assumed to be $t + \\epsilon$, then instead of minimizing $L$ itself, we minimize the average, or expected, loss given by:\n",
    "$$\\mathbb{E}[L] = \\int \\int L(t, f(\\mathbf{x})) p(\\mathbf{x}, t) d\\mathbf{x} dt$$\n",
    "If we choose the squared-error loss function, then this expression becomes:\n",
    "$$\\mathbb{E}[L] = \\int \\int [f(\\mathbf{x}) - t]^2 p(\\mathbf{x}, t) d\\mathbf{x} dt$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown, through the *calculus of variations* (discussed in Appendix B) that the optimal choice of predictive function is:\n",
    "$$f^*(\\mathbf{x}) = \\mathbb{E}_t[t|\\mathbf{x}]$$\n",
    "For a Gaussian conditional distribution of the form specified above (i.e. when the error terms are Gaussian), the conditional mean will simply be: $$\\mathbb{E}[t|\\mathbf{x}] = \\int tp(t|\\mathbf{x})dt = y(\\mathbf{y}, \\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
