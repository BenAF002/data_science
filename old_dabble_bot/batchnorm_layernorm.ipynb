{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, L = 2, 4, 6\n",
    "x1 = torch.randn(C, L)\n",
    "x2 = torch.randn(N, C, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystifying Batchnorm & Layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a tensor with dimensions $N$, $C$, $L$:\n",
    "$$ \\mu_C = \\frac{1}{NL} \\sum_{i=1}^{N} \\sum_{j=1}^{L} x_{iCj} \\\\ \\ \\\\ \\sigma_C^2 = \\frac{1}{NL} \\sum_{i=1}^N \\sum_{j=1}^L (x_{iCj} - \\mu_C)^2 \\\\ \\ \\\\ \\hat{x} = \\frac{x - \\mu_c}{\\sqrt{\\sigma_C^2 + \\epsilon}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4146,  0.5744,  0.2266,  0.3660, -0.0236,  0.7007],\n",
       "         [-0.1406, -0.5448, -1.9718, -1.2147,  0.6173,  0.1095],\n",
       "         [ 0.6311, -0.1577, -0.5117, -1.2719,  0.5677,  1.0311],\n",
       "         [ 1.2559, -0.1773, -1.2809,  0.3551,  0.5591, -2.0358]],\n",
       "\n",
       "        [[ 0.3466,  0.1193, -2.5732,  1.1626, -1.3817,  0.8970],\n",
       "         [ 1.1253,  0.2677,  0.2603,  2.0825, -0.2255, -0.3650],\n",
       "         [ 1.9877, -0.9347, -0.3170, -1.7596,  0.2667,  0.4683],\n",
       "         [ 0.7194,  1.2219, -0.3333, -0.2791, -0.9944,  0.9895]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = torch.mean(x2, [0, 2], keepdim=True)  # x2.sum(0).sum(1) / (N * L)\n",
    "sig2 = torch.var(x2, [0, 2], correction=0, keepdim=True)  # no correction for bias (Bessel's correction) per equation above\n",
    "xhat = (x2 - mu) / torch.sqrt(sig2 + 1e-5)\n",
    "xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4146,  0.5744,  0.2266,  0.3660, -0.0236,  0.7007],\n",
       "         [-0.1406, -0.5448, -1.9718, -1.2147,  0.6173,  0.1095],\n",
       "         [ 0.6311, -0.1577, -0.5117, -1.2719,  0.5677,  1.0311],\n",
       "         [ 1.2559, -0.1773, -1.2809,  0.3551,  0.5591, -2.0358]],\n",
       "\n",
       "        [[ 0.3466,  0.1193, -2.5732,  1.1626, -1.3817,  0.8970],\n",
       "         [ 1.1253,  0.2677,  0.2603,  2.0825, -0.2255, -0.3650],\n",
       "         [ 1.9877, -0.9347, -0.3170, -1.7596,  0.2667,  0.4683],\n",
       "         [ 0.7194,  1.2219, -0.3333, -0.2791, -0.9944,  0.9895]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(4)\n",
    "bn(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layernorm *just* normalizes over the last dimension $L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7468,  0.8993, -0.0312,  0.3420, -0.7006,  1.2373],\n",
       "         [ 0.4467, -0.0241, -1.6857, -0.8041,  1.3292,  0.7379],\n",
       "         [ 0.7450, -0.2630, -0.7154, -1.6868,  0.6640,  1.2561],\n",
       "         [ 1.3171,  0.0387, -0.9458,  0.5136,  0.6955, -1.6192]],\n",
       "\n",
       "        [[ 0.4425,  0.2705, -1.7667,  1.0599, -0.8652,  0.8590],\n",
       "         [ 0.7118, -0.3038, -0.3125,  1.8454, -0.8879, -1.0531],\n",
       "         [ 1.7311, -0.7539, -0.2286, -1.4554,  0.2677,  0.4391],\n",
       "         [ 0.6205,  1.2459, -0.6893, -0.6219, -1.5118,  0.9566]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = x2.mean(2, keepdim=True)\n",
    "sig2 = x2.var(2, keepdim=True, unbiased=False)\n",
    "\n",
    "xhat = (x2 - mu) / torch.sqrt(sig2 + 1e-5)\n",
    "xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7468,  0.8993, -0.0312,  0.3420, -0.7006,  1.2373],\n",
       "         [ 0.4467, -0.0241, -1.6857, -0.8041,  1.3292,  0.7379],\n",
       "         [ 0.7450, -0.2630, -0.7154, -1.6868,  0.6640,  1.2561],\n",
       "         [ 1.3171,  0.0387, -0.9458,  0.5136,  0.6955, -1.6192]],\n",
       "\n",
       "        [[ 0.4425,  0.2705, -1.7667,  1.0599, -0.8652,  0.8590],\n",
       "         [ 0.7118, -0.3038, -0.3125,  1.8454, -0.8879, -1.0531],\n",
       "         [ 1.7311, -0.7539, -0.2286, -1.4554,  0.2677,  0.4391],\n",
       "         [ 0.6205,  1.2459, -0.6893, -0.6219, -1.5118,  0.9566]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.LayerNorm(6)\n",
    "ln(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "Similar to BatchNorm. BatchNorm calculates the sample statistics (mean and variance) over dim=0 of the input batch. This means that the BatchNorm is calculating the sample statistics across samples in the batch. LayerNorm calculates the sample statistics over dim=1 of the input batch. This means that LayerNorm calculates sample statistics for individual samples.\n",
    "\n",
    "More precisely, BatchNorm calculates sample statistics over all elements of all instances (samples) in a batch for each feature independently. Whereas LayerNorm calculates sample statistics across the features and elements for each instance (sample) independently.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "So, what we get with LayerNorm is unit-Gaussian (standardized) features for each token. The LayerNorm dimensions are (B, T) - we have T unit-Gaussian tokens for each of the B samples in the batch. At least, they are unit-Gaussian at initialization. They have trainable *gamma* and *beta* parameters (scale and shift) parameters like BatchNorm does, so after training they may have a different, learned, distribution.\n",
    "\n",
    "**Note:** Because LayerNorm statistics are independent across samples in a batch, LayerNorm does not need to use buffers (running mean and running var); the statistics may be calculated the same way at any time regardless of batch size. This alos means that LayerNorm will work the same way during evaluation as during training.\n",
    "\n",
    "One reason why LayerNorm may be preferred in this architecture is because BatchNorm can be bad for NLP applications (as I so painfully learnt working with RNNs in makemore). This is because variable sequence lengths can lead to variable batch lengths, which produces variation in BatchNorm sample statistics, leading to instability during training. \n",
    "\n",
    "#### Deviation in Model Architecture\n",
    "We will implement the LayerNorms before each layer (stack/feed-forward) of the network instead of after. Andrej says that this is more often the standard practice today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
