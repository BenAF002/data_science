{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as a\n",
    "import time\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abaft',\n",
       " 'abalone',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandonment']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('C:\\\\Users\\\\benak\\\\Documents\\\\More Documents\\\\words.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  (n) the vertical force exerted by a mass as a result of gravity\n",
      "2:  (n) sports equipment used in calisthenic exercises and weightlifting; it is not attached to anything and is raised and lowered by use of the hands and arms\n",
      "3:  (n) the relative importance granted to something\n",
      "4:  (n) an artifact that is heavy\n",
      "5:  (n) an oppressive feeling of heavy force\n",
      "6:  (n) a system of units used to express the weight of something\n",
      "7:  (n) a unit used to measure weight\n",
      "8:  (n) (statistics) a coefficient assigned to elements of a frequency distribution in order to represent their relative importance\n",
      "9:  (v) weight down with a load\n",
      "10:  (v) present with a bias\n"
     ]
    }
   ],
   "source": [
    "# sample definitions for the word: weight\n",
    "syns = wn.synsets('weight')\n",
    "num = 0\n",
    "for s in syns:\n",
    "    num += 1\n",
    "    print(f'{num}: ', f'({s.pos()})', s.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Words Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n",
      "54010\n"
     ]
    }
   ],
   "source": [
    "wsample = []\n",
    "undefined = []\n",
    "ix = torch.randperm(len(words))\n",
    "for ix in ix: wsample.append(words[ix])\n",
    "for w in wsample:\n",
    "    if len(wn.synsets(w)) < 1: \n",
    "        undefined.append(w)      # create set of words without a wordnet definition \n",
    "        wsample.remove(w)        # remove undefined words from the sample\n",
    "        \n",
    "print(len(undefined))\n",
    "print(len(wsample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions for all words in wsample\n",
    "definitions = [s.definition() for w in wsample for s in wn.synsets(w)]\n",
    "\n",
    "# remove punctuations from definitions and append ' . ' sentence-end token\n",
    "trim_definitions = [''.join(d).translate(str.maketrans('', '', string.punctuation)) + ' . ' for d in definitions]\n",
    "trim_defstring = ''.join(trim_definitions)    # join punctuation-free definitions into a string\n",
    "\n",
    "# create vocab list of all individual words that appear in the sample set of definitions\n",
    "vocab = sorted(list((dict.fromkeys(trim_defstring.split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions in sample: \n",
      "  161831\n",
      "Distinct words in sampled definitions: \n",
      "  1482928\n",
      "Unique words in sampled definitions: \n",
      "  Vocab set: 29593\n"
     ]
    }
   ],
   "source": [
    "print('Definitions in sample: \\n ', len(definitions))\n",
    "print('Distinct words in sampled definitions: \\n ', len(trim_defstring.split()))\n",
    "print('Unique words in sampled definitions: \\n  Vocab set:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4260"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of rare words in the sample vocab set (words appearing only once in the sample of definitions)\n",
    "counts = []\n",
    "for word in vocab:\n",
    "    counts += [trim_defstring.count(word)]\n",
    "\n",
    "idk = []\n",
    "for i in range(len(counts)):\n",
    "    if counts[i] < 2: idk.append(i)\n",
    "rare_words = [vocab[ix] for ix in idk]\n",
    "len(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a large mass of ice floating at sea usually broken off of a polar glacier ',\n",
       " ' Austrian composer in music system ',\n",
       " ' a formal message requesting something that is submitted to an authority ',\n",
       " ' reverent petition to a deity ',\n",
       " ' write a petition for something to somebody request formally and in writing ',\n",
       " ' telephone central where circuits are completed with patchcords ',\n",
       " ' the quality of being exact ',\n",
       " ' a researcher who works in the field ',\n",
       " ' in a focal manner ',\n",
       " ' food chopped into small bits ']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rare words from the sample definitions\n",
    "trimmer_defs = trim_defstring.split()\n",
    "for w in rare_words:\n",
    "    trimmer_defs.remove(w)\n",
    "trimmer_defs = ' '.join(trimmer_defs).split('.')\n",
    "trimmer_defs = [d for d in trimmer_defs]\n",
    "trimmer_defs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab set excl rare words: 25332\n"
     ]
    }
   ],
   "source": [
    "# remove all rare words from the vocab list\n",
    "trim_vocab = sorted(list((dict.fromkeys(' '.join(trimmer_defs).split()))))\n",
    "print('Vocab set excl rare words:', len(trim_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316837"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''.join(trimmer_defs).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_char = '.'\n",
    "start_char = '<s>'\n",
    "pad_char = '<p>'\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(trim_vocab)}    # word-to-integer mapping dictionary\n",
    "stoi[end_char] = len(stoi) + 1                     # adding end character\n",
    "stoi[start_char] = len(stoi) + 2                   # adding start character\n",
    "stoi[pad_char] = 0                                 # adding pad character\n",
    "itos = {i:s for s,i in stoi.items()}               # integer-to-word mapping dictionary1\n",
    "\n",
    "enc = lambda s: [stoi[c] for c in s]            # encoder\n",
    "dec = lambda l: ' '.join([itos[i] for i in l])  # decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Population Summaries:\n",
    "- Total Vocab Size: 29,593\n",
    "- Vocab Size Excluding Rare Words (1 offs): 25,333\n",
    "- Total Definitions: 161,831\n",
    "- Total Words: 1,482,928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets\n",
    "Naively encode the entire dataset into one tensor object - for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([25335,  2560, 13933, 14824, 16153, 12443, 10616,  3875, 20165, 24273,\n",
       "          4988, 16154, 16153,  2560, 17529, 11325,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([ 2560, 13933, 14824, 16153, 12443, 10616,  3875, 20165, 24273,  4988,\n",
       "         16154, 16153,  2560, 17529, 11325, 25333,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [enc(d.split()) for d in trimmer_defs]\n",
    "max_length = max([len(d) for d in data])\n",
    "xdat = [enc([start_char]) + d for d in data]\n",
    "ydat = [d + enc([end_char]) for d in data]\n",
    "\n",
    "# right pad all definitions to max length\n",
    "for d in xdat: d += [0] * (max_length - len(d) + 1)\n",
    "for d in ydat: d += [0] * (max_length - len(d) + 1)\n",
    "\n",
    "xdat = torch.tensor(xdat)\n",
    "ydat = torch.tensor(ydat)\n",
    "\n",
    "xdat[0], ydat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "Xt, Yt = xdat[:n], ydat[:n]     # 80% training data\n",
    "Xv, Yv = xdat[n:], ydat[n:]     # 20% validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vs = len(stoi) + 1\n",
    "bts = 128       # batch size\n",
    "bls = max_length + 1      # block size\n",
    "n_emb = 256     # embedding dimesions\n",
    "n_head = 8     # number of heads per multihead stack (head_size = n_emb // n_head)\n",
    "n_layer = 8    # number of decoder blocks\n",
    "dropout = 0.2  # probability of zeroing-out neuron in dropouts\n",
    "learning_rate = 1e-3\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def minibatch(split):\n",
    "    if split == 'train':\n",
    "        xdat, ydat = Xt, Yt\n",
    "    else:\n",
    "        xdat, ydat = Xv, Yv\n",
    "    ix = torch.randint(len(xdat) - bls, (bts,))\n",
    "    \n",
    "    x = xdat[ix]\n",
    "    y = ydat[ix]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y    \n",
    "\n",
    "xb, yb = minibatch(tdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(bls, bls)))  # (T, T)\n",
    "            # torch.tril() is not a parameter, so we have to use register_buffer to assign it to the module\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B, T, hs)\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # (B, T, T); scaled by 1/sqrt(hs)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)        \n",
    "        \n",
    "        # aggregate values by weights\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Self-Attention Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])    # create list of heads\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_emb)    # linear transformation of the output from the head stack\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)    # feed forward through heads and concatenate output\n",
    "        out = self.dropout(self.proj(out))                     # pass output through linear layer and dropout\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),     # mult 4 bc the paper does a 4x channel expansion in the feedforward\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.sa = MultiHead(n_head, head_size)    # self-attention stack\n",
    "        self.ffwd = FeedForward(n_emb)\n",
    "        self.ln1 = nn.LayerNorm(n_emb)   # layer normalization for self-attention stack\n",
    "        self.ln2 = nn.LayerNorm(n_emb)   # layer normalization for feed forward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))     # residual self-attention stack connection\n",
    "        x = x + self.ffwd(self.ln2(x))   # residual feed-forward connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vs, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(bls, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_emb)   # final layer norm\n",
    "        self.lm_head = nn.Linear(n_emb, vs)    # output linear layer\n",
    "        \n",
    "        \n",
    "    def forward(self, input, targets=None):\n",
    "        B, T = input.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(input)                               # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))   # (T,C)\n",
    "        x = tok_emb + pos_emb                                                     # (B,T,C)\n",
    "        x = self.blocks(x)                                                        # (B,T,C)\n",
    "        x = self.ln_f(x)                                                          # (B,T,C)\n",
    "        logits = self.lm_head(x)                                                  # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=0)    # ignore index of pad_char\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, samples):    # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        model.eval()\n",
    "        sample = []\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            ctx = idx\n",
    "            \n",
    "            while True:\n",
    "                ctx_cond = ctx[:, -bls:]\n",
    "                logits, loss = self(ctx_cond)\n",
    "\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                ctx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "                # append sampled index to the running sequence\n",
    "                ctx = torch.cat((ctx, ctx_next), dim=1) # (B, T+1)\n",
    "                \n",
    "                if ctx_next.item() == stoi[end_char] or ctx.shape[1] > 50:\n",
    "                    break\n",
    "            sample.append(dec(ctx.tolist()[0]))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = minibatch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model = DefModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 5.1810, val loss 5.2918               | ETA: 29.46 min\n",
      "step 1000: train loss 4.4424, val loss 4.7023               | ETA: 27.76 min\n",
      "step 1500: train loss 3.8014, val loss 4.2540               | ETA: 26.36 min\n",
      "step 2000: train loss 3.3022, val loss 3.9345               | ETA: 24.94 min\n",
      "step 2500: train loss 2.9824, val loss 3.6862               | ETA: 23.63 min\n",
      "step 3000: train loss 2.6932, val loss 3.4661               | ETA: 21.70 min\n",
      "step 3500: train loss 2.4770, val loss 3.3373               | ETA: 20.26 min\n",
      "step 4000: train loss 2.3257, val loss 3.2073               | ETA: 19.00 min\n",
      "step 4500: train loss 2.1809, val loss 3.1505               | ETA: 17.05 min\n",
      "step 5000: train loss 2.1001, val loss 3.0594               | ETA: 15.50 min\n",
      "step 5500: train loss 2.0301, val loss 3.0457               | ETA: 14.25 min\n",
      "step 6000: train loss 1.9272, val loss 2.9527               | ETA: 12.40 min\n",
      "step 6500: train loss 1.8783, val loss 2.9301               | ETA: 10.97 min\n",
      "step 7000: train loss 1.8218, val loss 2.9017               | ETA: 9.25 min\n",
      "step 7500: train loss 1.7802, val loss 2.8714               | ETA: 7.75 min\n",
      "step 8000: train loss 1.7375, val loss 2.8098               | ETA: 6.30 min\n",
      "step 8500: train loss 1.7082, val loss 2.8601               | ETA: 4.63 min\n",
      "step 9000: train loss 1.6735, val loss 2.8265               | ETA: 3.10 min\n",
      "step 9500: train loss 1.6526, val loss 2.7610               | ETA: 1.58 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "max_iters = 10000\n",
    "eval_iters = 500\n",
    "tloss, vloss = [], []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    start = time.time()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter > 0 and iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        tloss.append(losses['train'])\n",
    "        vloss.append(losses['val'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} \\\n",
    "              | ETA: {run_time / 60 * (max_iters-iter):.2f} min\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = minibatch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    run_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the training loss and validation loss diverge. We clearly suffer from overfitting. The dataset still has many low frequency words - there are certainly words included in the validation set that are not included in the training set. As we become better at producing predictions for words (and definitions) in the training set, the impact of these unseen words within the validation set becomes more disruptive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> someone who flees from an uncongenial situation .',\n",
       " '<s> issue commands or orders for .',\n",
       " '<s> United States manufacturer of automobiles who pioneered mass production .',\n",
       " '<s> a joyful occasion for special festivities to mark some happy event .',\n",
       " '<s> stop amount .',\n",
       " '<s> make similar in sound .',\n",
       " '<s> hot or cold alcoholic mixed drink containing a beaten egg .',\n",
       " '<s> lift and laborious because of restraint or sensation .',\n",
       " '<s> transfer too much .',\n",
       " '<s> be owned by be in the possession of .',\n",
       " '<s> a movement downward .',\n",
       " '<s> running lengthwise .',\n",
       " '<s> a disposition to exhibit uncontrolled anger .',\n",
       " '<s> a humorous anecdote or remark intended to provoke laughter .',\n",
       " '<s> state or say further .',\n",
       " '<s> a poem consisting of 3 stanzas and an envoy .',\n",
       " '<s> attentively .',\n",
       " '<s> protection from harm .',\n",
       " '<s> a woman who works the right for a game .',\n",
       " '<s> plot a map of land .']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor([[stoi[start_char]]], device=device)\n",
    "model.generate(context, samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> inquire into .',\n",
       " '<s> increase in value or to a higher point .',\n",
       " '<s> be adjacent or come together .',\n",
       " '<s> likely to attract attention .',\n",
       " '<s> not having a roof .',\n",
       " '<s> put forward as of an idea .',\n",
       " '<s> incapable of being .',\n",
       " '<s> protect or defend a position in order to reach a game .',\n",
       " '<s> use dental floss to clean .',\n",
       " '<s> composing as in an idea .',\n",
       " '<s> a farm that gathers .',\n",
       " '<s> computer science a set of data on which samples changes feed or a number of issues of data is changed from which come to which one previously been paid .',\n",
       " '<s> equip with a fuse provide with a fuse .',\n",
       " '<s> move through by or as if by whistling .',\n",
       " '<s> deem wrong or inappropriate .',\n",
       " '<s> the act of communicating with a deity especially as a petition or in adoration or contrition or thanksgiving .',\n",
       " '<s> make a rupture in the ranks of the enemy or ones own by quitting or fleeing .',\n",
       " '<s> have as a as a will hold .',\n",
       " '<s> the scent of a greasy glandular secretion from the male musk deer .',\n",
       " '<s> an announcement in a dispute where you pay for your purchases .']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor([[stoi[start_char]]], device=device)\n",
    "model.generate(context, samples=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
